{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64ed7a1",
   "metadata": {},
   "source": [
    "# This notebook produces spectrograms which represent a collection of decimated WAV files. These spectrograms will be stored in the \"data\" directory. A LST file is also produced, which contains each spectrogram's annotation information. Finally, either a REC file or Augmented Manifest file is created for the purpose of model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8fd38a",
   "metadata": {},
   "source": [
    "#### Make sure that the kernel is either \"Python 3 Data Science\" (for SageMaker Studio) or \"conda_amazonei_pytorch_latest_p37\" (for SageMaker Notebook Instances)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb06af6",
   "metadata": {},
   "source": [
    "#### Change the name of the S3 Bucket (wherever it appears in the code) to reflect the name of the S3 Bucket in your AWS Account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b0fa3",
   "metadata": {},
   "source": [
    "## Installs and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972baf78",
   "metadata": {},
   "source": [
    "OPTIONAL: Upgrades \"pip\" (only run this code chunk once each time you start the notebook instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb782a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I recommend leaving this commented out since package installation in this notebook is very finicky.\n",
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b739f1",
   "metadata": {},
   "source": [
    "Installs the \"librosa\" library for spectrogram creation (always needed; must run exactly once each time you start the notebook instance).\n",
    "If the installation processes \"locks up\", perform the following steps: log out of JupyterLab, stop and start the notebook instance again, reopen JupyterLab, and try again."
   ]
  },
  {
   "cell_type": "code",
   "id": "6abbc65f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T20:06:51.592914Z",
     "start_time": "2024-05-21T20:06:32.287812Z"
    }
   },
   "source": [
    "# This takes roughly five minutes to fully install when it is your first time ever running it, but it is absolutely necessary.\n",
    "    # (It should only take a couple of minutes to install every other time.)\n",
    "!conda install -y -c conda-forge librosa"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\r\n",
      "Channels:\r\n",
      " - conda-forge\r\n",
      " - defaults\r\n",
      "Platform: osx-arm64\r\n",
      "Collecting package metadata (repodata.json): done\r\n",
      "Solving environment: done\r\n",
      "\r\n",
      "## Package Plan ##\r\n",
      "\r\n",
      "  environment location: /opt/anaconda3\r\n",
      "\r\n",
      "  added / updated specs:\r\n",
      "    - librosa\r\n",
      "\r\n",
      "\r\n",
      "The following packages will be downloaded:\r\n",
      "\r\n",
      "    package                    |            build\r\n",
      "    ---------------------------|-----------------\r\n",
      "    audioread-3.0.1            |  py311h267d04e_1          44 KB  conda-forge\r\n",
      "    ffmpeg-4.3.2               |       h38cfed3_3         9.3 MB  conda-forge\r\n",
      "    gettext-0.22.5             |       h8fbad5d_2         471 KB  conda-forge\r\n",
      "    gettext-tools-0.22.5       |       h8fbad5d_2         2.4 MB  conda-forge\r\n",
      "    gnutls-3.6.13              |       h706517b_1         2.0 MB  conda-forge\r\n",
      "    lame-3.100                 |    h1a8c8d9_1003         516 KB  conda-forge\r\n",
      "    libasprintf-0.22.5         |       h8fbad5d_2          40 KB  conda-forge\r\n",
      "    libasprintf-devel-0.22.5   |       h8fbad5d_2          34 KB  conda-forge\r\n",
      "    libcxx-17.0.6              |       h5f092b4_0         1.2 MB  conda-forge\r\n",
      "    libflac-1.4.3              |       hb765f3a_0         307 KB  conda-forge\r\n",
      "    libgettextpo-0.22.5        |       h8fbad5d_2         156 KB  conda-forge\r\n",
      "    libgettextpo-devel-0.22.5  |       h8fbad5d_2          36 KB  conda-forge\r\n",
      "    libiconv-1.17              |       h0d3ecfb_2         661 KB  conda-forge\r\n",
      "    libintl-0.22.5             |       h8fbad5d_2          79 KB  conda-forge\r\n",
      "    libintl-devel-0.22.5       |       h8fbad5d_2          38 KB  conda-forge\r\n",
      "    libogg-1.3.4               |       h27ca646_1         203 KB  conda-forge\r\n",
      "    libopus-1.3.1              |       h27ca646_1         247 KB  conda-forge\r\n",
      "    librosa-0.10.2.post1       |     pyhd8ed1ab_0         194 KB  conda-forge\r\n",
      "    libsndfile-1.2.2           |       h9739721_1         310 KB  conda-forge\r\n",
      "    libvorbis-1.3.7            |       h9f76cd9_0         249 KB  conda-forge\r\n",
      "    mpg123-1.32.6              |       hebf3989_0         351 KB  conda-forge\r\n",
      "    nettle-3.6                 |       hc6a1b29_0         1.2 MB  conda-forge\r\n",
      "    openh264-2.1.1             |       habe5f53_0         1.3 MB  conda-forge\r\n",
      "    openssl-3.3.0              |       hfb2fe0b_2         2.8 MB  conda-forge\r\n",
      "    pooch-1.8.1                |     pyhd8ed1ab_0          51 KB  conda-forge\r\n",
      "    pysoundfile-0.12.1         |     pyhd8ed1ab_0          27 KB  conda-forge\r\n",
      "    soxr-0.1.3                 |       h5008568_3          88 KB  conda-forge\r\n",
      "    soxr-python-0.3.7          |  py311h9ea6feb_0         242 KB  conda-forge\r\n",
      "    x264-1!161.3030            |       h3422bc3_1         2.0 MB  conda-forge\r\n",
      "    ------------------------------------------------------------\r\n",
      "                                           Total:        26.3 MB\r\n",
      "\r\n",
      "The following NEW packages will be INSTALLED:\r\n",
      "\r\n",
      "  audioread          conda-forge/osx-arm64::audioread-3.0.1-py311h267d04e_1 \r\n",
      "  ffmpeg             conda-forge/osx-arm64::ffmpeg-4.3.2-h38cfed3_3 \r\n",
      "  gettext-tools      conda-forge/osx-arm64::gettext-tools-0.22.5-h8fbad5d_2 \r\n",
      "  gnutls             conda-forge/osx-arm64::gnutls-3.6.13-h706517b_1 \r\n",
      "  lame               conda-forge/osx-arm64::lame-3.100-h1a8c8d9_1003 \r\n",
      "  libasprintf        conda-forge/osx-arm64::libasprintf-0.22.5-h8fbad5d_2 \r\n",
      "  libasprintf-devel  conda-forge/osx-arm64::libasprintf-devel-0.22.5-h8fbad5d_2 \r\n",
      "  libflac            conda-forge/osx-arm64::libflac-1.4.3-hb765f3a_0 \r\n",
      "  libgettextpo       conda-forge/osx-arm64::libgettextpo-0.22.5-h8fbad5d_2 \r\n",
      "  libgettextpo-devel conda-forge/osx-arm64::libgettextpo-devel-0.22.5-h8fbad5d_2 \r\n",
      "  libintl            conda-forge/osx-arm64::libintl-0.22.5-h8fbad5d_2 \r\n",
      "  libintl-devel      conda-forge/osx-arm64::libintl-devel-0.22.5-h8fbad5d_2 \r\n",
      "  libogg             conda-forge/osx-arm64::libogg-1.3.4-h27ca646_1 \r\n",
      "  libopus            conda-forge/osx-arm64::libopus-1.3.1-h27ca646_1 \r\n",
      "  librosa            conda-forge/noarch::librosa-0.10.2.post1-pyhd8ed1ab_0 \r\n",
      "  libsndfile         conda-forge/osx-arm64::libsndfile-1.2.2-h9739721_1 \r\n",
      "  libvorbis          conda-forge/osx-arm64::libvorbis-1.3.7-h9f76cd9_0 \r\n",
      "  mpg123             conda-forge/osx-arm64::mpg123-1.32.6-hebf3989_0 \r\n",
      "  nettle             conda-forge/osx-arm64::nettle-3.6-hc6a1b29_0 \r\n",
      "  openh264           conda-forge/osx-arm64::openh264-2.1.1-habe5f53_0 \r\n",
      "  pooch              conda-forge/noarch::pooch-1.8.1-pyhd8ed1ab_0 \r\n",
      "  pysoundfile        conda-forge/noarch::pysoundfile-0.12.1-pyhd8ed1ab_0 \r\n",
      "  soxr               conda-forge/osx-arm64::soxr-0.1.3-h5008568_3 \r\n",
      "  soxr-python        conda-forge/osx-arm64::soxr-python-0.3.7-py311h9ea6feb_0 \r\n",
      "  x264               conda-forge/osx-arm64::x264-1!161.3030-h3422bc3_1 \r\n",
      "\r\n",
      "The following packages will be UPDATED:\r\n",
      "\r\n",
      "  gettext              pkgs/main::gettext-0.21.0-h13f89a0_1 --> conda-forge::gettext-0.22.5-h8fbad5d_2 \r\n",
      "  libcxx                pkgs/main::libcxx-14.0.6-h848a8c0_0 --> conda-forge::libcxx-17.0.6-h5f092b4_0 \r\n",
      "  libiconv              pkgs/main::libiconv-1.16-h1a28f6b_2 --> conda-forge::libiconv-1.17-h0d3ecfb_2 \r\n",
      "  openssl                                  3.3.0-h0d3ecfb_0 --> 3.3.0-hfb2fe0b_2 \r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Downloading and Extracting Packages:\r\n",
      "ffmpeg-4.3.2         | 9.3 MB    |                                       |   0% \r\n",
      "openssl-3.3.0        | 2.8 MB    |                                       |   0% \u001B[A\r\n",
      "\r\n",
      "gettext-tools-0.22.5 | 2.4 MB    |                                       |   0% \u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "x264-1!161.3030      | 2.0 MB    |                                       |   0% \u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "gnutls-3.6.13        | 2.0 MB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "openh264-2.1.1       | 1.3 MB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "nettle-3.6           | 1.2 MB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libcxx-17.0.6        | 1.2 MB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libiconv-1.17        | 661 KB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "lame-3.100           | 516 KB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "gettext-0.22.5       | 471 KB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "mpg123-1.32.6        | 351 KB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libsndfile-1.2.2     | 310 KB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libflac-1.4.3        | 307 KB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libvorbis-1.3.7      | 249 KB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libopus-1.3.1        | 247 KB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "soxr-python-0.3.7    | 242 KB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libogg-1.3.4         | 203 KB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "librosa-0.10.2.post1 | 194 KB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libgettextpo-0.22.5  | 156 KB    |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "soxr-0.1.3           | 88 KB     |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libintl-0.22.5       | 79 KB     |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "pooch-1.8.1          | 51 KB     |                                       |   0% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "ffmpeg-4.3.2         | 9.3 MB    | 4                                     |   1% [A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "gnutls-3.6.13        | 2.0 MB    | 5                                     |   2% \u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "openssl-3.3.0        | 2.8 MB    | #8                                    |   5% \u001B[A\r\n",
      "\r\n",
      "ffmpeg-4.3.2         | 9.3 MB    | ####7                                 |  13% \u001B[A\u001B[A\r\n",
      "openssl-3.3.0        | 2.8 MB    | ##########8                           |  29% \u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "gnutls-3.6.13        | 2.0 MB    | ###5                                  |  10% \u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "ffmpeg-4.3.2         | 9.3 MB    | ######7                               |  18% \u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "gnutls-3.6.13        | 2.0 MB    | ###############                       |  41% \u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "openssl-3.3.0        | 2.8 MB    | ###############                       |  41% \u001B[A\r\n",
      "\r\n",
      "\r\n",
      "ffmpeg-4.3.2         | 9.3 MB    | ###########6                          |  32% \u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "gettext-tools-0.22.5 | 2.4 MB    | ##############6                       |  40% \u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "gnutls-3.6.13        | 2.0 MB    | ################################5     |  88% \u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "x264-1!161.3030      | 2.0 MB    | #################6                    |  48% \u001B[A\u001B[A\u001B[A\r\n",
      "ffmpeg-4.3.2         | 9.3 MB    | ##############7                       |  40% \u001B[A\r\n",
      "\r\n",
      "ffmpeg-4.3.2         | 9.3 MB    | #################8                    |  48% \u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "openh264-2.1.1       | 1.3 MB    | 4                                     |   1% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "openssl-3.3.0        | 2.8 MB    | #######################6              |  64% \u001B[A\r\n",
      "\r\n",
      "gettext-tools-0.22.5 | 2.4 MB    | ##################################### | 100% \u001B[A\u001B[A\r\n",
      "openssl-3.3.0        | 2.8 MB    | ###########################6          |  75% \u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "nettle-3.6           | 1.2 MB    | 4                                     |   1% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "openh264-2.1.1       | 1.3 MB    | ###6                                  |  10% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "ffmpeg-4.3.2         | 9.3 MB    | ####################7                 |  56% \u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "nettle-3.6           | 1.2 MB    | ##########4                           |  28% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "openh264-2.1.1       | 1.3 MB    | ########6                             |  23% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "openssl-3.3.0        | 2.8 MB    | ###############################2      |  84% \u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "gnutls-3.6.13        | 2.0 MB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "ffmpeg-4.3.2         | 9.3 MB    | ########################8             |  67% \u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "nettle-3.6           | 1.2 MB    | ################################7     |  88% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "openssl-3.3.0        | 2.8 MB    | ###################################   |  95% \u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "ffmpeg-4.3.2         | 9.3 MB    | ###########################7          |  75% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "openssl-3.3.0        | 2.8 MB    | ##################################### | 100% \u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "openh264-2.1.1       | 1.3 MB    | #################################7    |  91% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libcxx-17.0.6        | 1.2 MB    | 4                                     |   1% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libiconv-1.17        | 661 KB    | 8                                     |   2% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "nettle-3.6           | 1.2 MB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "ffmpeg-4.3.2         | 9.3 MB    | ##############################3       |  82% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "x264-1!161.3030      | 2.0 MB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "openh264-2.1.1       | 1.3 MB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libiconv-1.17        | 661 KB    | ################1                     |  44% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libcxx-17.0.6        | 1.2 MB    | ###############                       |  41% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "gettext-0.22.5       | 471 KB    | #2                                    |   3% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "ffmpeg-4.3.2         | 9.3 MB    | #################################8    |  91% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libcxx-17.0.6        | 1.2 MB    | ##########################6           |  72% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libiconv-1.17        | 661 KB    | ###############################3      |  85% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "gettext-0.22.5       | 471 KB    | ############################8         |  78% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "ffmpeg-4.3.2         | 9.3 MB    | ####################################7 |  99% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libiconv-1.17        | 661 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "mpg123-1.32.6        | 351 KB    | #6                                    |   5% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libflac-1.4.3        | 307 KB    | #9                                    |   5% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libvorbis-1.3.7      | 249 KB    | ##3                                   |   6% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libsndfile-1.2.2     | 310 KB    | #9                                    |   5% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libopus-1.3.1        | 247 KB    | ##3                                   |   6% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "mpg123-1.32.6        | 351 KB    | #######################6              |  64% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libflac-1.4.3        | 307 KB    | ###################2                  |  52% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "soxr-python-0.3.7    | 242 KB    | ##4                                   |   7% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libsndfile-1.2.2     | 310 KB    | ##########################7           |  72% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libogg-1.3.4         | 203 KB    | ##9                                   |   8% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "librosa-0.10.2.post1 | 194 KB    | ###                                   |   8% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libgettextpo-0.22.5  | 156 KB    | ###7                                  |  10% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libintl-0.22.5       | 79 KB     | #######4                              |  20% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "soxr-0.1.3           | 88 KB     | ######7                               |  18% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "pooch-1.8.1          | 51 KB     | ###########4                          |  31% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      " ... (more hidden) ...\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libcxx-17.0.6        | 1.2 MB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libcxx-17.0.6        | 1.2 MB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libvorbis-1.3.7      | 249 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libvorbis-1.3.7      | 249 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libopus-1.3.1        | 247 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libopus-1.3.1        | 247 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "soxr-python-0.3.7    | 242 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "soxr-python-0.3.7    | 242 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libflac-1.4.3        | 307 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libflac-1.4.3        | 307 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "mpg123-1.32.6        | 351 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "mpg123-1.32.6        | 351 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libsndfile-1.2.2     | 310 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "gettext-0.22.5       | 471 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "librosa-0.10.2.post1 | 194 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "librosa-0.10.2.post1 | 194 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libintl-0.22.5       | 79 KB     | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libintl-0.22.5       | 79 KB     | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libogg-1.3.4         | 203 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libogg-1.3.4         | 203 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "soxr-0.1.3           | 88 KB     | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "soxr-0.1.3           | 88 KB     | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libgettextpo-0.22.5  | 156 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "libgettextpo-0.22.5  | 156 KB    | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "pooch-1.8.1          | 51 KB     | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "pooch-1.8.1          | 51 KB     | ##################################### | 100% \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      " ... (more hidden) ...\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "ffmpeg-4.3.2         | 9.3 MB    | ##################################### | 100% [A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \r\n",
      "                                                                                \u001B[A\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                \u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\u001B[A\r\n",
      "\r\n",
      "\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001B[A\u001B[A\u001B[A\u001B[A\u001B[A\r\n",
      "Preparing transaction: done\r\n",
      "Verifying transaction: done\r\n",
      "Executing transaction: done\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "df7c5462",
   "metadata": {},
   "source": [
    "Installs the \"mxnet\" library for REC file creation (currently always needed; must run exactly once each time you start the notebook instance)"
   ]
  },
  {
   "cell_type": "code",
   "id": "7abf6aae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T20:08:41.484073Z",
     "start_time": "2024-05-21T20:08:33.522557Z"
    }
   },
   "source": [
    "# This is absolutely necessary, but the installation should only take less than a minute every time.\n",
    "!pip install mxnet"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\r\n",
      "  Downloading mxnet-1.6.0-py2.py3-none-any.whl.metadata (3.4 kB)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /opt/anaconda3/lib/python3.11/site-packages (from mxnet) (1.26.4)\r\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /opt/anaconda3/lib/python3.11/site-packages (from mxnet) (2.31.0)\r\n",
      "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\r\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl.metadata (6.4 kB)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.20.0->mxnet) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.20.0->mxnet) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.20.0->mxnet) (2024.2.2)\r\n",
      "Downloading mxnet-1.6.0-py2.py3-none-any.whl (68.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m68.7/68.7 MB\u001B[0m \u001B[31m10.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\r\n",
      "Installing collected packages: graphviz, mxnet\r\n",
      "Successfully installed graphviz-0.8.4 mxnet-1.6.0\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "3dfaa884",
   "metadata": {},
   "source": [
    "Properly installs the \"opencv\" library for use in im2rec.py (only needed if working in SageMaker Studio)"
   ]
  },
  {
   "cell_type": "code",
   "id": "79af8ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T20:08:54.239465Z",
     "start_time": "2024-05-21T20:08:54.235807Z"
    }
   },
   "source": [
    "\"\"\"WARNING: Only run this if you are working in SageMaker Studio, not if you are working in SageMaker Notebook Instances.\"\"\"\n",
    "#!pip install opencv-python-headless"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WARNING: Only run this if you are working in SageMaker Studio, not if you are working in SageMaker Notebook Instances.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "054d7b1c",
   "metadata": {},
   "source": [
    "Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "id": "6f4a9a79",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-30T21:49:39.579876Z",
     "start_time": "2024-05-30T21:49:39.574702Z"
    }
   },
   "source": [
    "# Package Imports (leave in this order)\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.io import wavfile\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from os import path\n",
    "import boto3\n",
    "from PIL import Image\n",
    "import json\n",
    "from os.path import exists\n",
    "import io"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "37a590f5",
   "metadata": {},
   "source": [
    "## Specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe9ed1",
   "metadata": {},
   "source": [
    "Constants and Variables"
   ]
  },
  {
   "cell_type": "code",
   "id": "a19e6570",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-30T21:49:39.587148Z",
     "start_time": "2024-05-30T21:49:39.581233Z"
    }
   },
   "source": [
    "# Original annotation files' column names\n",
    "left_col, right_col = \"Begin Time (s)\", \"End Time (s)\"\n",
    "top_col, bot_col = \"High Freq (Hz)\", \"Low Freq (Hz)\"\n",
    "class_col = \"Species\"\n",
    "\n",
    "# Files and Directories\n",
    "# Note that you should create this \"data\" folder in the same directory as this notebook.\n",
    "output_dir = \"./data\"\n",
    "\n",
    "# Desired format for the final data file (either \"REC\" or \"AugmentedManifest\")\n",
    "file_format = \"REC\"\n",
    "\n",
    "# SPECTROGRAM CONSTANTS\n",
    "# Window size (n_fft) in seconds\n",
    "WINDOW_SIZE_SEC = 3/20\n",
    "# Hop Length in seconds\n",
    "HOP_LEN_SEC = 15/300\n",
    "# Number of frequency bands (y dimension of spectrogram)\n",
    "N_MELS = 300\n",
    "# Maximum frequency considered (highest value in y dimension)\n",
    "FREQUENCY_MAX = 1600\n",
    "# Length of one chunk in seconds\n",
    "CHUNK_SIZE_SEC = 30\n",
    "# Amount of overlap between subsequent spectrograms (in seconds)\n",
    "SPEC_OVERLAP = 3.0\n",
    "# Proportion of spectrogram that reflects the minimum area hand-annotation boxes will be allowed to have\n",
    "MIN_BOX_AREA = 0.0006\n",
    "\n",
    "# Dictionary mapping annotators' inconsistent species labels to consistent species labels (i.e., class labels)\n",
    "    # All species labels found so far: \n",
    "    # {nan, '?', 'sl', 'rf', 'KW', 'hhb', 'jn', 'hb3', '3l', 'SL', 'sl3', 'kw?', 'hb?', 'al', 'lw', 's;', 'hb', 'kw', 'Hb', 'hn', 'hb ', 'jhb'}\n",
    "\n",
    "CLASS_LABEL_MAP = {\n",
    "        \"humpback whale\": \"hb\",\n",
    "        \"hb whale\": \"hb\",\n",
    "        \"hb?\": \"hb\",\n",
    "        \"hhb\": \"hb\",\n",
    "        \"hb\": \"hb\",\n",
    "        \"hb3\": \"hb\",\n",
    "        \"Hb\": \"hb\",\n",
    "        \"jhb\": \"hb\",\n",
    "        \"jn\": \"hb\",\n",
    "        \"hn\": \"hb\",\n",
    "        \"hb \": \"hb\",\n",
    "        \"hbn\": \"hb\",\n",
    "    \n",
    "        \"killer whale\": \"kw\",\n",
    "        \"kw\": \"kw\",\n",
    "        \"kw \": \"kw\",\n",
    "        \"KW\": \"kw\",\n",
    "        \"kw?\": \"kw\",\n",
    "        \"lw\": \"kw\",\n",
    "    \n",
    "        \"rockfish\": \"rf\",\n",
    "        \"rf\": \"rf\",\n",
    "\n",
    "        \"sea lion\": \"sl\",\n",
    "        \"sl\": \"sl\",\n",
    "        \"sl \": \"sl\",\n",
    "        \"SL\": \"sl\",\n",
    "        \"s;\": \"sl\",\n",
    "        \"al\": \"sl\",\n",
    "        \"3l\": \"sl\",\n",
    "        \"sl3\": \"sl\",\n",
    "    \n",
    "        \"mech\": \"mech\",\n",
    "        \"mech \": \"mech\",\n",
    "        \"mech.\": \"mech\",\n",
    "        \"mechanical\": \"mech\",\n",
    "    \n",
    "        \"?\": \"?\",\n",
    "        \"? \": \"?\"\n",
    "    }\n",
    "\n",
    "# Specifies which classes' annotations to remove when preparing the annotation data\n",
    "DISALLOWED_CLASSES = [\"?\", \"mech\"]"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "89a087b9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da6f5b0",
   "metadata": {},
   "source": [
    "Creates a connection to the S3 Bucket where the decimated WAV files and annotation files are stored."
   ]
  },
  {
   "cell_type": "code",
   "id": "3a355798",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-30T21:49:51.607792Z",
     "start_time": "2024-05-30T21:49:39.588670Z"
    }
   },
   "source": [
    "def awsKeys(file):\n",
    "    awsKeys = pd.read_csv(file)\n",
    "    access_key = awsKeys['Access key ID'][0]\n",
    "    secret_key = awsKeys['Secret access key'][0]\n",
    "    return access_key, secret_key\n",
    "\n",
    "\n",
    "def clientAndBucket(file, region='us-west-2'):\n",
    "    aws_access_key_id, aws_secret_access_key = awsKeys(file)\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        region_name=region\n",
    "    )\n",
    "    bucket_name = 'whale-recordings'\n",
    "    s3 = boto3.resource('s3',\n",
    "                        aws_access_key_id=aws_access_key_id,\n",
    "                        aws_secret_access_key=aws_secret_access_key,\n",
    "                        region_name=region\n",
    "                        )\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    return s3_client, bucket\n",
    "\n",
    "\n",
    "KEYS = \"ssundar_accessKeys.csv\"\n",
    "s3_client, bucket = clientAndBucket(KEYS)\n",
    "\n",
    "%run model_functions.ipynb\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# List to store processed data\n",
    "processed_data = []\n",
    "D2 = []\n",
    "backgroundFiles = []\n",
    "\n",
    "path = \"CPhydrophone/Avila/Deployment 2/selection-tables/\"\n",
    "\n",
    "keys = [obj.key for obj in bucket.objects.all()]\n",
    "selectionTables = [(obj.split(\"/\")[-1], obj) for obj in keys if path in obj][1:]\n",
    "\n",
    "KEYS = \"ssundar_accessKeys.csv\"\n",
    "s3_client, bucket = clientAndBucket(KEYS)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished preprocessing\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "9ecb261b",
   "metadata": {},
   "source": [
    "## Download Script\n",
    "\n",
    "Downloads all **39** audio files and the corresponding annotated .txt files from the S3 bucket\n",
    "\n",
    "to a folder named \"files\" "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:50:15.289682Z",
     "start_time": "2024-05-30T21:50:04.717112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "wavPath = \"CPhydrophone/Avila/Deployment 2/wav-files/decimated_files/\"\n",
    "backgroundFiles = []\n",
    "! cd files\n",
    "! find . -type f \\( -name \"*.wav\" -o -name \"*-SS.txt\" \\) -exec rm {} +\n",
    "! cd ..\n",
    "FOLDER = \"files_aws\"\n",
    "os.makedirs(FOLDER, exist_ok=True)\n",
    "for item in tqdm(selectionTables):\n",
    "    try:\n",
    "        ss = item[0]\n",
    "        # wav = ss.split(\"-SS.txt\")[0] + \"_processed.wav\"\n",
    "        wav = ss.split(\"-SS.txt\")[0]\n",
    "        p1 = os.path.join('files', ss)\n",
    "        p2 = os.path.join('files', wav)\n",
    "        if not os.path.exists(p1):\n",
    "            s3_client.download_file(bucket_name, item[1], f'{FOLDER}/{ss}')\n",
    "        if not os.path.exists(p2):\n",
    "            s3_client.download_file(bucket_name, wavPath + wav, f'{FOLDER}/{wav}')\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "len(selectionTables)"
   ],
   "id": "90fabbff44210f23",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:10<00:00,  3.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:30:13.336559Z",
     "start_time": "2024-05-30T21:30:13.333205Z"
    }
   },
   "cell_type": "code",
   "source": "# len([f for f in os.listdir('files') if f.endswith('_processed.wav')])/",
   "id": "17afee867d5825ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "ea66971c",
   "metadata": {
    "tags": []
   },
   "source": [
    "def read_wavfile(wav_name, normalize=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Reads in a decimated wav file from the S3 Bucket.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "        wav_name: string\n",
    "            Numeric portion of the decimated WAV file's name\n",
    "        normalize: boolean\n",
    "            Indicates whether or not to normalize the sound data (i.e., the amplitudes of the sound wave recorded by the hydrophone)\n",
    "        verbose: boolean\n",
    "            Indicates whether or not to make output excessively detailed\n",
    "    ----------\n",
    "    \n",
    "    RETURNS\n",
    "    ----------\n",
    "        sr: int\n",
    "            Sampling rate of WAV file\n",
    "        data: numpy array\n",
    "            Contains floats representing the amplitudes of the sound wave for each sample (automatically ordered from earliest to latest)\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    # Downloads the decimated WAV file from the S3 Bucket where it is stored.\n",
    "    wav_name = np.random.choice([file for file in os.listdir('files') if file.endswith('_processed.wav')])\n",
    "    file_name = f\"{wav_name}_processed.wav\"\n",
    "    bucket_path = f\"CPhydrophone/Avila/Deployment 2/wav-files/decimated_files/{file_name}\"\n",
    "    # bucket.download_file(bucket_path, f'files/{file_name}')\n",
    "    print(\"read_wav\", file_name)\n",
    "    \n",
    "    # Reads-in the decimated WAV file's information.\n",
    "    if verbose:\n",
    "        print(\"Reading {}\".format(file_name))     \n",
    "    sr, data = wavfile.read(file_name)\n",
    "        # NOTE: Sampling rate (sr) seems to be 8000 samples per second\n",
    "    print(sr)\n",
    "    \n",
    "    # Removes the WAV file from our working directory since we have obtained the information we need\n",
    "    os.remove(file_name)\n",
    "    \n",
    "    # Normalizes the decimated WAV file and returns important information\n",
    "    if verbose:\n",
    "        print(\"{} samples at {} samples/sec --> {} seconds\".format(data.shape[0], sr, data.shape[0]/sr))\n",
    "    if normalize:\n",
    "        data = data.astype(float)\n",
    "        data = data - data.min()\n",
    "        data = data / data.max()\n",
    "        data = data - 0.5\n",
    "    return sr, data\n",
    "\n",
    "def read_annotations(fname, verbose=False):\n",
    "    \"\"\"\n",
    "    Given the name of a WAV file (fname), tries to find the corresponding annotation file (i.e., selection table) in the S3 Bucket.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "        fname: string\n",
    "            Numeric portion of the TXT file's name (i.e., the TXT file which contains the WAV file's annotations)\n",
    "        verbose: boolean\n",
    "            Indicates whether or not to make output excessively detailed\n",
    "    ----------\n",
    "    \n",
    "    RETURNS\n",
    "    ----------\n",
    "        annotations: pandas dataframe\n",
    "            Contains information on the hand-annotation boxes (i.e., \"ground truth\" information) for the given WAV file.\n",
    "            This data frame is derived from a \"Raven\" selection table (each row is an annotation box).\n",
    "            The professor's \"Bio Team\" makes each annotation by drawing a selection box around an animal vocalization using the \"Raven\" app.\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    # Establishes the initials for first name and last name of every single annotator so far\n",
    "        # NOTE: Add to this list if new annotators join the team.\n",
    "    annotators = ['AS.txt', 'AW.txt', 'JW.txt', 'MS.txt', 'SS.txt']\n",
    "    \n",
    "    # Finds the annotation files corresponding to \"fname\" and creates a list containing their names\n",
    "    # annot_files = []\n",
    "    # for annotator in annotators:\n",
    "    #     file_name = f\"{fname}-{annotator}\"\n",
    "    #         # EXAMPLE: file_name = f\"{fname}-AW.txt\"\n",
    "    #     bucket_path = f\"CPhydrophone/Avila/Deployment 2/selection-tables/{file_name}\"\n",
    "    #     try:\n",
    "    #         print(bucket_path)\n",
    "    #         bucket.download_file(bucket_path, file_name)\n",
    "    #         annot_files.append(file_name)\n",
    "    #     except Exception as e:\n",
    "    #         print(e)\n",
    "    #         continue\n",
    "    \n",
    "    annotators = ['AS.txt', 'AW.txt', 'JW.txt', 'MS.txt', 'SS.txt']\n",
    "    annot_files = []\n",
    "    for annotator in annotators:\n",
    "        file_name = f\"{fname}-{annotator}\"\n",
    "        local_path = f\"{file_name}\"\n",
    "        if os.path.exists(local_path):\n",
    "            annot_files.append(local_path)\n",
    "        else:\n",
    "            print(f\"File {local_path} does not exist.\")\n",
    "            continue\n",
    "    \n",
    "    print(len(annot_files))\n",
    "    # Accounts for the misnamed annotation file which includes the word \"txt\" twice\n",
    "    if len(annot_files) == 0 and fname == \"671658014.181007063421\":\n",
    "        print(\"File not found. Assuming it is misnamed.\")\n",
    "        file_name = \"671658014.181007063421-AStxt.txt\"\n",
    "        bucket_path = f\"selection-tables/{file_name}\"\n",
    "        try:\n",
    "            bucket.download_file(bucket_path, file_name)\n",
    "            annot_files.append(file_name)\n",
    "        except Exception:\n",
    "            print(\"Did not find misnamed file.\")\n",
    "            exit(1)\n",
    "    \n",
    "    # Gets the \"better\" annotation file out of the two that correspond to the WAV file called \"671658014.180929213545.wav\"\n",
    "    if len(annot_files) == 2 and fname == \"671658014.180929213545\":\n",
    "        print(\"Two annotation files found. Using AW's annotations.\")\n",
    "        os.remove(\"671658014.180929213545-SS.txt\")\n",
    "        file_name = \"671658014.180929213545-AW.txt\"\n",
    "        bucket_path = f\"selection-tables/{file_name}\"\n",
    "        try:\n",
    "            bucket.download_file(bucket_path, file_name)\n",
    "            annot_files = [file_name]\n",
    "        except Exception:\n",
    "            print(\"Did not find AW's annotations.\")\n",
    "            exit(1)\n",
    "    \n",
    "    # Gets the \"better\" annotation file out of the two that correspond to the decimated WAV file called \"671658014.180929033558.wav\"\n",
    "    if len(annot_files) == 2 and fname == \"671658014.180929033558\":\n",
    "        print(\"Two annotation files found. Using AS's annotations.\")\n",
    "        os.remove(\"671658014.180929033558-JW.txt\")\n",
    "        file_name = \"671658014.180929033558-AS.txt\"\n",
    "        bucket_path = f\"selection-tables/{file_name}\"\n",
    "        try:\n",
    "            bucket.download_file(bucket_path, file_name)\n",
    "            annot_files = [file_name]\n",
    "        except Exception:\n",
    "            print(\"Did not find AS's annotations.\")\n",
    "            exit(1)\n",
    "    \n",
    "    # Takes the \"list\" of annotation file names (which really only contains one name) and reads its information as a PANDAS data frame\n",
    "    print(annot_files)\n",
    "    annots = []\n",
    "    for file_name in annot_files:\n",
    "        file = pd.read_csv(file_name, sep=\"\\t\")\n",
    "        \n",
    "        # Corrects a known mispelling of the \"Species\" column name in one of the WAV files\n",
    "        try:\n",
    "            len(file[class_col])\n",
    "        except Exception as e:\n",
    "            file.rename(columns = {\"Spcies\": \"Species\"}, inplace = True)\n",
    "        \n",
    "        # Adds the data frame to a \"list\" of the WAV file's annotation data frames (which really only contains one data frame)\n",
    "        annots.append(file)\n",
    "    \n",
    "    \n",
    "    # Catches any situations where an unexpected number of annotation files is found\n",
    "    if len(annots) == 0:\n",
    "        print(\"ERROR: File not found. Terminating Program.\")\n",
    "        exit(1)\n",
    "    elif len(annots) == 1:\n",
    "        annotations = annots[0]\n",
    "    else:\n",
    "        print(\"There are multiple annotation files for this WAV file. It is unclear which one you wish to use.\")\n",
    "        exit(1)\n",
    "        \n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Read {} annotations from {}\".format(len(annotations), fname))\n",
    "        print(\"Columns:\", \",\".join([\" {} ({})\".format(c, type(c)) for c in annotations.columns]))\n",
    "    \n",
    "    # Removes the annotation files from our working directory since we have the information we need\n",
    "    for file_name in annot_files:\n",
    "        os.remove(file_name)\n",
    "    return annotations\n",
    "\n",
    "randomFile = np.random.choice([file for file in os.listdir('files') if file.endswith('_processed.wav')])\n",
    "read_wavfile(f'files/{randomFile}', verbose=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8dbb77ac",
   "metadata": {},
   "source": [
    "Defines and calls the function that creates the training, validation, and testing datasets. It gets all the decimated WAV files from the relevant S3 Bucket. Then, it sets aside the hardcoded WAV files reserved for testing and validation purposes. The remaining files become the training data."
   ]
  },
  {
   "cell_type": "code",
   "id": "8104f406",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-30T21:36:17.716372Z",
     "start_time": "2024-05-30T21:36:12.589967Z"
    }
   },
   "source": [
    "def get_data_sets():\n",
    "    \"\"\"\n",
    "    Gets the file names associated with the training data, validation data, and testing data.\n",
    "        *Note that the \"validation\" data is used to tune parameters, while the \"testing\" data provides an unbiased estimate of model performance.\n",
    "        \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "        N/A\n",
    "    ----------\n",
    "    \n",
    "    RETURNS\n",
    "    ----------\n",
    "        train_set: list of strings\n",
    "            Contains the names (numeric portions only) of the WAV files which belong to the training dataset.\n",
    "        validation_set: list of strings\n",
    "            Contains the names (numeric portions only) of the WAV files which belong to the validation dataset.\n",
    "        testing_set: list of strings\n",
    "            Contains the names (numeric portions only) of the WAV files which belong to the testing dataset.\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    \n",
    "    # Testing Set: Represents \"new data\" after the model has been trained/validated\n",
    "    # testing_set = ['671658014.181008003414', '671658014.180929003601']\n",
    "    # 2024 Training Set\n",
    "    files = [f for f in os.listdir('files') if f.endswith('_processed.wav')]\n",
    "    train_set = np.random.choice(files, int(len(files)*0.8), replace=False)\n",
    "    validation_set = np.random.choice([file for file in files if file not in train_set], int(len(files)*0.1), replace=False)\n",
    "    testing_set = [file for file in files if file not in train_set and file not in validation_set]\n",
    "    # Capstone Team's testing set\n",
    "    #testing_set = ['671658014.181008003414']\n",
    "\n",
    "    # Validation Set: Evaluates the model after training and helps tune post-processing parameters\n",
    "    # validation_set = ['671658014.181008033412', '671658014.180930183532']\n",
    "    # Capstone Team's validation set\n",
    "    #validation_set = ['671658014.181008033412']\n",
    "    \n",
    "    # Gets all the dataset names from the S3 Bucket\n",
    "    annotatedFiles = [file.key.split(\"/\")[1] for file in bucket.objects.all() if (file.key[-1] != '/' and \n",
    "                                                                                  file.key.split(\"/\")[0] == \"selection-tables\")]\n",
    "    dataset = [file.split(\"-\")[0] for file in annotatedFiles]\n",
    "\n",
    "    # Training Set: Teaches the model how to make predictions\n",
    "        # NOTE: It contains any data that is not in the testing set or validation set\n",
    "    # notAllowedSet = testing_set + validation_set\n",
    "    # train_set = [file for file in dataset if all(file not in notAllowed for notAllowed in notAllowedSet)]\n",
    "    \n",
    "    return train_set, validation_set, testing_set\n",
    "\n",
    "train_set, validation_set, testing_set = get_data_sets()\n",
    "len(train_set), len(validation_set), len(testing_set)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 3, 4)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "id": "cd1bf9b7",
   "metadata": {},
   "source": [
    "Displays the WAV file names that belong to the training data (with and without duplicate names removed)"
   ]
  },
  {
   "cell_type": "code",
   "id": "0981a978",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-30T21:21:58.893535Z",
     "start_time": "2024-05-30T21:21:58.891451Z"
    }
   },
   "source": [
    "# Duplicate names indicate the presence of multiple annotation files for a single WAV file (only one will get used)\n",
    "print(\"Size with Duplicates: \", len(train_set))\n",
    "print(train_set)\n",
    "print()\n",
    "\n",
    "# Removes duplicate names from the training set\n",
    "train_set = list(set(train_set))\n",
    "print(\"Size without Duplicates: \", len(train_set))\n",
    "print(train_set)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size with Duplicates:  30\n",
      "['6805.230204003826_processed.wav' '6805.230203210826_processed.wav'\n",
      " '6805.230206030826_processed.wav' '6805.230204180826_processed.wav'\n",
      " '6805.230206163827_processed.wav' '6805.230205183826_processed.wav'\n",
      " '6805.230206090827_processed.wav' '6805.230207000827_processed.wav'\n",
      " '6805.230203090826_processed.wav' '6805.230205210826_processed.wav'\n",
      " '6805.230204210826_processed.wav' '6805.230203110826_processed.wav'\n",
      " '6805.230202030825_processed.wav' '6805.230206233827_processed.wav'\n",
      " '6805.230202100825_processed.wav' '6805.230205180826_processed.wav'\n",
      " '6805.230206000826_processed.wav' '6805.230205090826_processed.wav'\n",
      " '6805.230202120825_processed.wav' '6805.230202150825_processed.wav'\n",
      " '6805.230203000825_processed.wav' '6805.230201210825_processed.wav'\n",
      " '6805.230205000826_processed.wav' '6805.230201180825_processed.wav'\n",
      " '6805.230207043827_processed.wav' '6805.230205150826_processed.wav'\n",
      " '6805.230202180825_processed.wav' '6805.230204120826_processed.wav'\n",
      " '6805.230201150825_processed.wav' '6805.230202000825_processed.wav']\n",
      "\n",
      "Size without Duplicates:  30\n",
      "['6805.230205183826_processed.wav', '6805.230203090826_processed.wav', '6805.230205090826_processed.wav', '6805.230203110826_processed.wav', '6805.230205150826_processed.wav', '6805.230206233827_processed.wav', '6805.230204003826_processed.wav', '6805.230201150825_processed.wav', '6805.230204120826_processed.wav', '6805.230201210825_processed.wav', '6805.230205210826_processed.wav', '6805.230202000825_processed.wav', '6805.230205000826_processed.wav', '6805.230204210826_processed.wav', '6805.230206090827_processed.wav', '6805.230203000825_processed.wav', '6805.230207000827_processed.wav', '6805.230201180825_processed.wav', '6805.230202180825_processed.wav', '6805.230204180826_processed.wav', '6805.230206163827_processed.wav', '6805.230207043827_processed.wav', '6805.230206030826_processed.wav', '6805.230202030825_processed.wav', '6805.230202120825_processed.wav', '6805.230202150825_processed.wav', '6805.230203210826_processed.wav', '6805.230205180826_processed.wav', '6805.230202100825_processed.wav', '6805.230206000826_processed.wav']\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "44718be2",
   "metadata": {},
   "source": [
    "Defines and calls the function that uses CLASS_LABEL_MAP to determine which species names are present in the annotation data.\n",
    "Also produces a dictionary that maps the correct class labels to unique class numbers."
   ]
  },
  {
   "cell_type": "code",
   "id": "ebada8ea",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-30T19:40:48.670222Z",
     "start_time": "2024-05-30T19:40:48.651426Z"
    }
   },
   "source": [
    "def get_all_classes(annotation_fnames, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns a list of all classes (i.e., species names) seen in the annotation files after class mapping, sorted alphabetically.\n",
    "    Note that this function does NOT alter the annotation files to reflect the class mapping (this occurs in a later function).\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "        annotation_fnames: list of strings\n",
    "            Contains the file names (numeric portions only) of the annotation files whose classes should be considered \n",
    "            when developing the class mapping\n",
    "        verbose: boolean\n",
    "            Indicates whether or not to make output excessively detailed\n",
    "    ----------\n",
    "    \n",
    "    RETURNS\n",
    "    ----------\n",
    "        clean_classes: set\n",
    "            After interpreting mislabeled species names as their correct names, contains every species name seen in annotation_fnames\n",
    "            (Since this is a set, it does not include duplicate species names.)\n",
    "            Names are sorted in alphabetical order.\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    # Gets every class name seen in annotation_fnames\n",
    "    classes = set()\n",
    "    for annot_fname in annotation_fnames:\n",
    "        try:\n",
    "            classes.update(list(read_annotations(annot_fname)[class_col].unique()))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    print(\"Raw classes: \", classes)\n",
    "    \n",
    "    # Combines class names that refer to the same animal into a single class name\n",
    "    clean_classes = set()\n",
    "    for cname in classes:\n",
    "        # Ignores annotations that lack a class name\n",
    "        if type(cname) == float:\n",
    "            continue\n",
    "        clean_classes.add(CLASS_LABEL_MAP[cname])\n",
    "    \n",
    "    # Sorts the class names in alphabetical order\n",
    "    clean_classes = sorted([s for s in list(clean_classes)])\n",
    "    if verbose:\n",
    "        print(\"Cleaned Class Names: \", clean_classes)\n",
    "    return clean_classes\n",
    "\n",
    "\n",
    "        \n",
    "# Gets all class names seen across the annotation files and removes undesired class names\n",
    "classes = get_all_classes(train_set+validation_set+testing_set, verbose=True)\n",
    "classes = [c for c in classes if c not in DISALLOWED_CLASSES]\n",
    "print(\"Allowed Classes: \", classes)\n",
    "\n",
    "# Produces dictionaries that map each \"Allowed Class\" to a unique class number (class numbering starts at 1)\n",
    "class_map = {}\n",
    "rev_class_map = {}\n",
    "for i in range(len(classes)):\n",
    "    # Class numbers are the keys and class names are the values\n",
    "        # UNUSED\n",
    "    class_map[i+1] = classes[i]\n",
    "    # Class names are the keys and class numbers are the values\n",
    "        # USED\n",
    "    rev_class_map[classes[i]] = i+1"
   ],
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('<U31'), dtype('<U31')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUFuncTypeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 49\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m clean_classes\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# Gets all class names seen across the annotation files and removes undesired class names\u001B[39;00m\n\u001B[0;32m---> 49\u001B[0m classes \u001B[38;5;241m=\u001B[39m get_all_classes(train_set\u001B[38;5;241m+\u001B[39mvalidation_set\u001B[38;5;241m+\u001B[39mtesting_set, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     50\u001B[0m classes \u001B[38;5;241m=\u001B[39m [c \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m classes \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m DISALLOWED_CLASSES]\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAllowed Classes: \u001B[39m\u001B[38;5;124m\"\u001B[39m, classes)\n",
      "\u001B[0;31mUFuncTypeError\u001B[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U31'), dtype('<U31')) -> None"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "465aac8f",
   "metadata": {},
   "source": [
    "Displays the class mapping that will get used."
   ]
  },
  {
   "cell_type": "code",
   "id": "83bf394f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T19:40:51.041503Z",
     "start_time": "2024-05-30T19:40:51.028543Z"
    }
   },
   "source": [
    "print(rev_class_map)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rev_class_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(rev_class_map)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'rev_class_map' is not defined"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "059cdb15",
   "metadata": {},
   "source": [
    "Defines a function that calculates the area of a given annotation box (measured as the proportion of the spectrogram it covers)."
   ]
  },
  {
   "cell_type": "code",
   "id": "d68c7199",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-30T19:40:53.459079Z",
     "start_time": "2024-05-30T19:40:53.456605Z"
    }
   },
   "source": [
    "def get_area(annotation):\n",
    "    \"\"\"\n",
    "    Given an annotated bounding box, returns the calculated area of the box.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "        annotation: row of a PANDAS data frame\n",
    "            Contains the information denoting a hand-annotated bounding box.\n",
    "    ----------\n",
    "    \n",
    "    RETURNS\n",
    "    ----------\n",
    "        A float denoting the calculated area of the box (relative to the full spectrogram, which has an area of 1).\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    return ((annotation[right_col] - annotation[left_col])\n",
    "            * (annotation[bot_col] - annotation[top_col]))"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "8c4e034e",
   "metadata": {},
   "source": [
    "Defines functions that use \"data chunks\" to create spectrograms and add their annotation data to the corresponding LST file.\n",
    "Each line of the LST file contains the annotation data for a given spectrogram, and the entire LST file contains the annotation data for a single dataset (such as the training set)."
   ]
  },
  {
   "cell_type": "code",
   "id": "d9680eb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T19:41:02.979936Z",
     "start_time": "2024-05-30T19:41:02.965066Z"
    }
   },
   "source": [
    "def process_file(wav_filename, annot_filename, min_bound, max_bound, chunk_size, lst_file_name, chunk_layout=\"dense\",\n",
    "                 drop_last_chunk=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Iterates through all data chunks in the current WAV file, serving as a shell for extract_chunk().\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "        wav_filename: string\n",
    "            Numeric portion of the decimated WAV file's name\n",
    "        annot_filename: string\n",
    "            Numeric portion of the TXT file's name (will correspond to \"wav_filename\")\n",
    "        min_bound: float \n",
    "            Used during spectrogram preparation within extract_chunk()\n",
    "        max_bound: float \n",
    "            Used during spectrogram preparation within extract_chunk()\n",
    "        chunk_size: int\n",
    "            Specifies the total elapsed time (in seconds) that each spectrogram will cover along its x-axis\n",
    "        lst_file_name: string\n",
    "            Specifies the name of the LST file (including the \".lst\" portion) that extract_chunk() should write its information to\n",
    "        chunk_layout: string\n",
    "            Either \"dense\" or \"sparse\".\n",
    "            It seems like the intention of this parameter was to modify the calculation of spectrogram overlap depending on how many \n",
    "                annotations were within the current spectrogram (i.e., the current chunk). However, its value is hardcoded as \"dense\", \n",
    "                and the code for spectrogram overlap has since been modified to where all spectrograms feature the same amount of \n",
    "                overlap regardless of chunk_layout.\n",
    "        drop_last_chunk: boolean\n",
    "            Boolean indicating whether or not to skip over the final spectrogram for the WAV file (\"False\" means, \"include the spectrogram\")\n",
    "        verbose: boolean\n",
    "            Indicates whether or not to make output excessively detailed\n",
    "    ----------\n",
    "    \n",
    "    RETURNS\n",
    "    ----------\n",
    "        examples: list of dictionaries\n",
    "            For each spectrogram, contains a dictionary mapping important spectrogram information (and annotation information) to \n",
    "                intuitive keywords.\n",
    "            Note that, within this function, some of the information gets written to the LST file.\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    # Reads-in WAV file information and annotation information\n",
    "        # For every decimated WAV file, sampling rate (sr) seems to be 8000 samples per second.\n",
    "    sr, data = read_wavfile(wav_filename, normalize=True, verbose=verbose)\n",
    "    annotations = read_annotations(annot_filename, verbose=verbose)\n",
    "    \n",
    "    # Converts spectrogram constants from being measured in seconds to being measured in samples\n",
    "    n_fft = int(WINDOW_SIZE_SEC * sr)\n",
    "    hop_len = int(HOP_LEN_SEC * sr)\n",
    "    chunk_size = int(chunk_size * sr)\n",
    "    \n",
    "    # Implements spectrogram overlap \n",
    "        # Example: \n",
    "        # If SPEC_OVERLAP = 3.0, the start time for one spectrogram is 3 seconds earlier than the end time of the previous spectrogram.\n",
    "    if chunk_layout == \"dense\":\n",
    "        step = chunk_size - int(SPEC_OVERLAP * sr)\n",
    "    elif chunk_layout == \"sparse\":\n",
    "        step = chunk_size - int(SPEC_OVERLAP * sr)\n",
    "    \n",
    "    # Builds a list of \"start values\", where each value specifies the index where a \"chunk's\" information starts in the \"data\" list\n",
    "        # Each chunk will be used to produce a single spectrogram.\n",
    "    start_vals = [s for s in range(0, len(data), step)]\n",
    "    \n",
    "    # Removes the last \"start value\" from the list if the corresponding chunk is too small\n",
    "    if len(data) - start_vals[-1] < int(chunk_size / 2):\n",
    "        start_vals = start_vals[:-1]\n",
    "        \n",
    "\n",
    "    def extract_chunk(start_i, end_i, spec_name, annot_name, json_name, index, use_pcen=True, M_init=None):\n",
    "        \"\"\"\n",
    "        Uses a single chunk of audio data to produce a spectrogram (and its corresponding line in the LST file).\n",
    "        This function gets called by process_file() multiple times until every spectrogram has been produced.\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            start_i: int\n",
    "                Specifies the index within the audio dataset where the current spectrogram begins\n",
    "                (Note that a value of 0 represents the first sample in the decimated WAV file.)\n",
    "            end_i: int\n",
    "                Specifies the index within the audio dataset where the next spectrogram begins\n",
    "                (Note that a value of len(data) represents the end of the decimated WAV file.)\n",
    "            spec_name: string\n",
    "                The name of the current spectrogram for when it gets saved to a PNG file (note that this includes the \".png\" portion).\n",
    "            annot_name: string\n",
    "                Currently-unused name for a TXT file which does not get created\n",
    "                (This nonexistant file would presumably contain the current spectrogram's annotations).\n",
    "            json_name: string\n",
    "                Currently-unused name for a JSON Lines file which does not get created\n",
    "                (This nonexistant file would presumably be the Augmented Manifest File for the current WAV file).\n",
    "            index: int\n",
    "                Indicates which spectrogram is currently being produced\n",
    "                (Note that this starts at 0 and increases by 1 with each subsequent spectrogram.\n",
    "                    This information is recorded within the LST file, so that it is clear which row corresponds to which spectrogram.)\n",
    "            use_pcen: boolean\n",
    "                Currently unused, but would indicate whether or not PCEN should be applied to the current spectrogram.\n",
    "            M_init: value (optional)\n",
    "                Would indicate the \"final filter delay value\" corresponding to the current spectrogram (for use in PCEN streaming).\n",
    "                    (Note that \"spectrogram\", \"chunk\", and \"block\" are presumably interchangeable terms in this context).\n",
    "                Since PCEN (let alone PCEN streaming) is currently unused, this is set to None.\n",
    "        ----------\n",
    "    \n",
    "        RETURNS\n",
    "        ----------\n",
    "            example_dict: dictionary\n",
    "                Maps important spectrogram information (and annotation information) to intuitive keywords for ease-of-access.\n",
    "                Note that, within this function, some of the information gets written to the LST file.\n",
    "            next_M_init: value (optional)\n",
    "                Would indicate the \"final filter delay value\" telling when to produce the next spectrogram (for use in PCEN streaming).\n",
    "                    (Note that \"spectrogram\", \"chunk\", and \"block\" are presumably interchangeable terms in this context).\n",
    "                Since PCEN (let alone PCEN streaming) is currently unused, this is set to None.\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        # Produces a \"spectrogram dataset\" for the current chunk\n",
    "        mel_spec = librosa.feature.melspectrogram(y=data[start_i:end_i],\n",
    "                                                  sr=sr,\n",
    "                                                  n_fft=n_fft,\n",
    "                                                  hop_length=hop_len,\n",
    "                                                  n_mels=N_MELS,\n",
    "                                                  fmax=FREQUENCY_MAX,\n",
    "                                                  center=False)\n",
    "        \n",
    "        # Attempt to Implement PCEN (Per-Channel Energy Normalization)\n",
    "            # SOURCE: https://librosa.org/doc/main/generated/librosa.pcen.html\n",
    "            # NOTE: Sampling rate is hardcoded as 8000 samples per second\n",
    "            # NOTE: \"hop_length\" seems to represent the number of audio samples within a 30-second spectrogram\n",
    "                # Calculation of \"hop_length\": \n",
    "                    # (8000 samples per second) * (60 seconds per minute) * (180 minutes per audio file) / (360 spectrograms per file)\n",
    "                    # Equals 240,000 samples per spectrogram\n",
    "                    # Assumes \"hop_length\" is simply the length of a 30-second spectrogram (hence the lack of spectrogram overlap)\n",
    "        #mel_spec = librosa.pcen(mel_spec, sr = 8000, hop_length = 240000, max_size = FREQUENCY_MAX)\n",
    "        # Using new value for hop_len that is more likely to be correct (equal to the hop len specified for the mel spectrogram)\n",
    "        #mel_spec = librosa.pcen(mel_spec * (2**31), sr = 8000, hop_length = hop_len, max_size = FREQUENCY_MAX)\n",
    "        # End of Attempt to Implement PCEN\n",
    "        \n",
    "        # Prepares current spectrogram\n",
    "        next_M_init = None\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            # In-Progress: PCEN Implementation\n",
    "        ###S = librosa.feature.melspectrogram(y=data[start_i:end_i], sr=sr, power = 1)\n",
    "        ###mel_spec = librosa.pcen(S * (2**31), sr = sr, hop_length = hop_len, max_size = FREQUENCY_MAX)\n",
    "        ###mel_spec = librosa.pcen(S, sr = sr, hop_length = hop_len, power = 1/4, time_constant = 0.7, max_size = FREQUENCY_MAX)\n",
    "            # End of PCEN Implementation\n",
    "        mel_spec = np.clip((mel_spec - min_bound) / (max_bound - min_bound) * 255, a_min=0, a_max=255)\n",
    "        mel_spec = mel_spec.astype(np.uint8)\n",
    "        spec_height, spec_width = mel_spec.shape\n",
    "        \n",
    "        # Creates a white horizontal stripe at the second highest frequency band on the current spectrogram\n",
    "        nrow, ncol = mel_spec.shape\n",
    "        for i in range(ncol):\n",
    "            # Draws the stripe on the spectrogram\n",
    "            mel_spec[nrow-2, i] = 255\n",
    "\n",
    "        # Gets annotions for the current chunk\n",
    "        start_s, end_s = start_i/sr, end_i/sr\n",
    "        freq_axis_low, freq_axis_high = librosa.hz_to_mel(0.0), librosa.hz_to_mel(FREQUENCY_MAX)\n",
    "        chunk_annotations = annotations.loc[~((annotations[left_col] > end_s)\n",
    "                                              | (annotations[right_col] < start_s))].copy()\n",
    "        print(start_s, end_s)\n",
    "        \n",
    "\n",
    "        # Rescale axes to 0.0-1.0 based on location inside chunk\n",
    "        chunk_annotations.loc[:,[left_col,right_col]] = ((chunk_annotations[[left_col,right_col]]\n",
    "                                                         - start_s) / (end_s - start_s))\n",
    "\n",
    "        chunk_annotations.loc[:,[bot_col,top_col]] = (1.0 - ((librosa.hz_to_mel(chunk_annotations[[bot_col,top_col]])\n",
    "                                                      - freq_axis_low) / (freq_axis_high - freq_axis_low)))\n",
    "        \n",
    "        \n",
    "        # Takes any mispelled or inconsistent annotation labels and replaces them with a consistent and correctly-spelled label\n",
    "        chunk_annotations[class_col] = chunk_annotations[class_col].map(CLASS_LABEL_MAP, na_action = \"ignore\")\n",
    "        # Code appears to filter the annotations based on which classes are allowed according to \"Allowed Classes\"\n",
    "        chunk_annotations = chunk_annotations.loc[chunk_annotations[class_col].isin(classes)]\n",
    "        \n",
    "        \n",
    "        # Clips hand-annotated bounding boxes that extend outside of the spectrogram (so that they end at the spectrogram's border)\n",
    "        trimmed_annots = chunk_annotations.copy()\n",
    "        trimmed_annots[left_col] = trimmed_annots[left_col].clip(lower=0, upper=1.0)\n",
    "        trimmed_annots[right_col] = trimmed_annots[right_col].clip(lower=0, upper=1.0)\n",
    "        trimmed_annots[bot_col] = trimmed_annots[bot_col].clip(lower=0, upper=1.0)\n",
    "        trimmed_annots[top_col] = trimmed_annots[top_col].clip(lower=0, upper=1.0)\n",
    "        \n",
    "        \n",
    "        # Implements a minimum area requirement for hand-annotated boxes\n",
    "        trimmed_box_areas = []\n",
    "        for i in trimmed_annots.index:\n",
    "            trimmed_box = trimmed_annots.loc[i]\n",
    "            trimmed_box_area = get_area(trimmed_box)\n",
    "            trimmed_box_areas.append(trimmed_box_area)\n",
    "        trimmed_annots[\"Trimmed_Box_Area\"] = trimmed_box_areas\n",
    "        trimmed_annots = trimmed_annots.loc[trimmed_annots[\"Trimmed_Box_Area\"] >= MIN_BOX_AREA]\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Found {} annotations in chunk\".format(len(chunk_annotations)))\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Saved spectrogram to '{}'\".format(spec_name))\n",
    "\n",
    "            \n",
    "        # Prepares spectrogram and annotation information to be written to files\n",
    "        image_filepath = path.join(output_dir, spec_name)\n",
    "        example_dict = {\n",
    "            \"filepath\": spec_name,\n",
    "            \"height\": spec_height,\n",
    "            \"width\": spec_width,\n",
    "            \"xmins\": trimmed_annots[left_col].tolist(),\n",
    "            \"xmaxs\": trimmed_annots[right_col].tolist(),\n",
    "            \"ymins\": trimmed_annots[top_col].tolist(),\n",
    "            \"ymaxs\": trimmed_annots[bot_col].tolist(),\n",
    "            \"classes_text\": trimmed_annots[class_col].tolist(),\n",
    "            \"classes\": trimmed_annots[class_col].map(rev_class_map).tolist()\n",
    "        }\n",
    "\n",
    "\n",
    "        # Saves chunk as PNG image (lossless compression)\n",
    "        im = Image.fromarray(mel_spec[::-1, :])\n",
    "        im = im.convert(\"L\")\n",
    "\n",
    "        image_filepath = path.join(output_dir, spec_name)\n",
    "        im.save(image_filepath)\n",
    "        \n",
    "        # Starts preparing the information for the LST file\n",
    "        res = [index, 2, 5]\n",
    "        for i in range(len(example_dict[\"xmins\"])):\n",
    "            # Skips \"flattened\" annotations, since they do not provide valuable information to the model\n",
    "                # NOTE: Assumes clipping has \"flattened\" annotations that are \"offscreen\"\n",
    "            if example_dict[\"ymins\"][i] == example_dict[\"ymaxs\"][i] or example_dict[\"xmins\"][i] == example_dict[\"xmaxs\"][i]:\n",
    "                continue\n",
    "            # Obtains annotation information:\n",
    "            temp = [example_dict[\"classes\"][i], example_dict[\"xmins\"][i], example_dict[\"ymins\"][i], \n",
    "                    example_dict[\"xmaxs\"][i], example_dict[\"ymaxs\"][i]]\n",
    "            res.extend(temp)\n",
    "            \n",
    "        # Creates an annotation for the horizontal stripe near the top of the spectrogram.\n",
    "            # Note that the \"stripe annotation\" (often called a \"blank annotation\") receives a class number of 0\n",
    "        BLANK_CLASS_NUM = 0\n",
    "        temp = [BLANK_CLASS_NUM, 0, 1/(2*N_MELS), 1, 5/(2*N_MELS)]\n",
    "        res.extend(temp)\n",
    "        # Updates \"example_dict\" to reflect the new blank annotation\n",
    "        new_xmins = [temp[1]] + example_dict[\"xmins\"]\n",
    "        new_ymins = [temp[2]] + example_dict[\"ymins\"]\n",
    "        new_xmaxs = [temp[3]] + example_dict[\"xmaxs\"]\n",
    "        new_ymaxs = [temp[4]] + example_dict[\"ymaxs\"]\n",
    "        new_classes_text = [\"blank\"] + example_dict[\"classes_text\"]\n",
    "        new_classes = [temp[0]] + example_dict[\"classes\"]\n",
    "        new_dict = {\n",
    "            \"filename\": spec_name,\n",
    "            \"height\": spec_height,\n",
    "            \"width\": spec_width,\n",
    "            \"xmins\": new_xmins,\n",
    "            \"xmaxs\": new_xmaxs,\n",
    "            \"ymins\": new_ymins,\n",
    "            \"ymaxs\": new_ymaxs,\n",
    "            \"classes_text\": new_classes_text,\n",
    "            \"classes\": new_classes\n",
    "        }\n",
    "        example_dict.update(new_dict)\n",
    "        \n",
    "        # Ensures that the file path to the current spectrogram is included in the LST file\n",
    "        res.append(image_filepath) \n",
    "\n",
    "        # Writes information to LST file\n",
    "        text = \"\\t\".join([str(el) for el in res])\n",
    "        with open(lst_file_name, \"a\") as f:\n",
    "            f.write(text)\n",
    "            f.write('\\n')\n",
    "\n",
    "        return example_dict, next_M_init\n",
    "    \n",
    "    # NOTE THAT \"extract_chunk()\" ENDS HERE \n",
    "    # ALSO NOTE THAT \"process_file()\" RESUMES HERE\n",
    "    \n",
    "    # Iterates through the WAV file, producing each chunk's spectrogram (and its line in the LST file) along the way\n",
    "    examples = []\n",
    "    M_init = None\n",
    "    for ind, start_i in enumerate(start_vals[:-1]):\n",
    "        spec_name = \"{}-{}.png\".format(wav_filename, ind)\n",
    "        annot_name = \"{}-{}-labels.txt\".format(wav_filename, ind)\n",
    "        json_name = f\"{wav_filename}.jsonl\"\n",
    "        ex, M_init = extract_chunk(start_i, start_i+chunk_size, spec_name, annot_name, json_name, ind, M_init=M_init)\n",
    "        examples.append(ex)\n",
    "    if not drop_last_chunk:\n",
    "        spec_name = \"{}-{}.png\".format(wav_filename, len(start_vals)-1)\n",
    "        annot_name = \"{}-{}-labels.txt\".format(wav_filename, len(start_vals)-1)\n",
    "        json_name = f\"{wav_filename}.jsonl\"\n",
    "        ex, _ = extract_chunk(start_vals[-1], len(data), spec_name, annot_name, json_name, len(start_vals)-1, M_init=M_init)\n",
    "        examples.append(ex)\n",
    "    else:\n",
    "        print(\"Dropping Last Chunk.\")\n",
    "    return examples"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "3c3cacc7",
   "metadata": {},
   "source": [
    "This function creates the LST file and obtains the information required to produce an Augmented Manifest file."
   ]
  },
  {
   "cell_type": "code",
   "id": "51bf1bec",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-30T19:41:05.126823Z",
     "start_time": "2024-05-30T19:41:05.123589Z"
    }
   },
   "source": [
    "def create_lst_file(dataset, lst_file_name):\n",
    "    \"\"\"\n",
    "    Creates spectrograms for the files included in \"dataset\" (storing them in the \"data\" directory) and creates the corresponding LST file.\n",
    "        *NOTE: Before this function gets called, create a folder called \"data\" (in the same directory as this notebook) for the spectrograms.\n",
    "        *NOTE: Make sure the \"data\" folder is empty before this function gets called. Calling cleanup() before create_data_files() will do this.\n",
    "        \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "        dataset: string\n",
    "            Specifies which dataset (train_set, validation_set, or testing_set) to produce a LST file for\n",
    "        lst_file_name: string\n",
    "            Specifies the name that you want the LST file to have (including the \".lst\" portion)\n",
    "    ----------\n",
    "    \n",
    "    RETURNS\n",
    "    ----------\n",
    "        aug_manif_info: two-dimensional list of dictionaries\n",
    "            For each spectrogram in the LST file, contains a dictionary mapping important spectrogram information (and annotation information) to \n",
    "                intuitive keywords.\n",
    "            Each list contains the dictionaries associated with a specific WAV file.\n",
    "                (These lists are all contained within one big list.)\n",
    "            This only gets used when producing an Augmented Manifest file, so it will not be used if file_format = \"REC\".\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    # Initializes important variables\n",
    "    index = 0\n",
    "    aug_manif_info = []\n",
    "    \n",
    "    # Iterates through the WAV file names in the \"dataset\"\n",
    "    for file in dataset:\n",
    "        # Displays progress update\n",
    "        print(f\"{index + 1}/{len(dataset)} wav files converted\")\n",
    "        # Increments index for next progress update\n",
    "        index += 1\n",
    "        \n",
    "        # Produces the spectrograms and LST file lines corresponding to the current WAV file\n",
    "        cur_info = process_file(file, file, -80.0, 0, CHUNK_SIZE_SEC, lst_file_name,chunk_layout=\"dense\", drop_last_chunk=False, verbose=False)\n",
    "        # Appends important spectrogram information and annotation information to the list\n",
    "        aug_manif_info.append(cur_info)\n",
    "        \n",
    "    return aug_manif_info"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "05ec0ba4",
   "metadata": {},
   "source": [
    "This function creates the REC file corresponding to the given LST file and the spectrograms in the \"data/\" directory.\n",
    "REC files are the recommended format for data during model training, and they contain everything necessary to reproduce the spectrograms and annotation boxes."
   ]
  },
  {
   "cell_type": "code",
   "id": "945ab341",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T19:41:08.733090Z",
     "start_time": "2024-05-30T19:41:08.730866Z"
    }
   },
   "source": [
    "def create_rec_file(lst_file_name):\n",
    "    \"\"\"\n",
    "    Runs the code contained within im2rec.py, creating a REC file that corresponds to the given LST file.\n",
    "    The REC file will have the same name as the LST file (with \".rec\" instead of \".lst\").\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "        lst_file_name: string\n",
    "            Specifies the name of the LST file (including the \".lst\" portion) that should be used to make the REC file.\n",
    "    ----------\n",
    "    \n",
    "    RETURNS\n",
    "    ----------\n",
    "        N/A\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    RESIZE_SIZE = 256\n",
    "    !python im2rec.py --resize $RESIZE_SIZE --pack-label $lst_file_name ."
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "f8ee6093",
   "metadata": {},
   "source": [
    "These functions create the Augmented Manifest file corresponding to the information returned from create_lst_file().\n",
    "\n",
    "Augmented Manifest Files are an alternative format for data during model training. They contain all annotation information as well as the file paths (relative to the S3 Bucket) for each spectrogram. This means that, if Augmented Manifest files are used, all spectrograms must be uploaded to the S3 Bucket (in the correct folder) before model training.\n",
    "\n",
    "Note that, so far, model training has only been successful using REC files."
   ]
  },
  {
   "cell_type": "code",
   "id": "66e8cd03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T19:41:11.291542Z",
     "start_time": "2024-05-30T19:41:11.286755Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "def create_json(file_prefix, spectro_with_annots):\n",
    "    \"\"\"\n",
    "    Creates a JSON object containing a single spectrogram's information along with the corresponding annotation information.\n",
    "    This JSON object is properly formatted to be a line in an Augmented Manifest File (JSON Lines file) for SageMaker training jobs.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "        file_prefix: string\n",
    "            Specifies the name that you want the Augmented Manifest file to have (without the \"_AugmentedManifestFile.jsonl\" portion).\n",
    "                (Corresponds to the LST file created at the same time as \"aug_manif_info\".)\n",
    "        spectro_with_annots: dictionary\n",
    "            Maps important spectrogram information (and annotation information) for the current spectrogram to intuitive keywords.\n",
    "    ----------\n",
    "    \n",
    "    RETURNS\n",
    "    ----------\n",
    "        json_obj: JSON Object\n",
    "            Contains a single spectrogram's information along with the corresponding annotation information.\n",
    "            Properly formatted to be a line in an Augmented Manifest File (JSON Lines file) for SageMaker training jobs.\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    # Creates intuitive variable names to reference important spectrogram information\n",
    "    spectro_name = spectro_with_annots[\"filename\"]\n",
    "    s3_location = f\"s3://sagemaker-us-west-2-************/{file_prefix}_manifest/spectrograms/{spectro_name}\"\n",
    "    width = spectro_with_annots[\"width\"]\n",
    "    height = spectro_with_annots[\"height\"]\n",
    "        # \"depth\" was found by downloading a spectrogram, opening it in the \"Photos\" application on Windows, and viewing file information.\n",
    "    depth = 8\n",
    "    \n",
    "    # Properly formats spectrogram size dimensions\n",
    "    image_size = [{\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "        \"depth\": depth\n",
    "    }]\n",
    "    \n",
    "    # Creates intuitive variable names to reference important annotation information\n",
    "    classes = spectro_with_annots[\"classes\"]\n",
    "    xmins = spectro_with_annots[\"xmins\"]\n",
    "    ymins = spectro_with_annots[\"ymins\"]\n",
    "    xmaxs = spectro_with_annots[\"xmaxs\"]\n",
    "    ymaxs = spectro_with_annots[\"ymaxs\"]\n",
    "    \n",
    "    # Properly formats annotation information, rescaling measurements to reflect the spectrogram's size dimensions\n",
    "    annotations = []\n",
    "    for i in range(len(spectro_with_annots[\"classes\"])):\n",
    "        cur_class = classes[i]\n",
    "        left = width*xmins[i]\n",
    "        top = height*ymins[i]\n",
    "        box_width = width*xmaxs[i]-left\n",
    "        box_height = height*ymaxs[i]-top\n",
    "        cur_annot = {\n",
    "            \"class_id\": cur_class, \n",
    "            \"left\": left,\n",
    "            \"top\": top,\n",
    "            \"width\": box_width,\n",
    "            \"height\": box_height\n",
    "        }\n",
    "        annotations.append(cur_annot)\n",
    "        \n",
    "    # Builds (and returns) a JSON object from the formatted information\n",
    "    boxes = {\"image_size\": image_size, \"annotations\": annotations}\n",
    "    json_obj_info = {\"spectrogram\": s3_location, \"boxes\": boxes}\n",
    "    json_obj = json.dumps(json_obj_info, indent=4)\n",
    "    return json_obj"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "7be49ea9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T19:41:13.155934Z",
     "start_time": "2024-05-30T19:41:13.151850Z"
    }
   },
   "source": [
    "def create_augmented_manifest_file(file_prefix, aug_manif_info):\n",
    "    \"\"\"\n",
    "    Iterates through every spectrogram in the LST file (specified by file_prefix).\n",
    "    Produces a JSON object for each spectrogram (using aug_manif_info).\n",
    "    Writes the JSON objects to a JSON Lines file.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "        file_prefix: string\n",
    "            Specifies the name that you want the Augmented Manifest file to have (without the \"_AugmentedManifestFile.jsonl\" portion).\n",
    "                (Corresponds to the LST file created at the same time as \"aug_manif_info\".)\n",
    "        aug_manif_info: two-dimensional list of dictionaries\n",
    "            For each spectrogram in the LST file, contains a dictionary mapping important spectrogram information (and annotation information) to \n",
    "                intuitive keywords.\n",
    "            Each list contains the dictionaries associated with a specific WAV file.\n",
    "                (These lists are all contained within one big list.)\n",
    "    ----------\n",
    "    \n",
    "    RETURNS\n",
    "    ----------\n",
    "        N/A\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    # Creates every JSON object and appends them to a list\n",
    "    all_json_objects = []\n",
    "    for wav_file_data in aug_manif_info:\n",
    "        for image_info in wav_file_data:\n",
    "            cur_json_obj = create_json(file_prefix, image_info)\n",
    "            all_json_objects.append(cur_json_obj)\n",
    "            \n",
    "    # Removes the previous Augmented Manifest File from the working directory (if one exists)\n",
    "    filename = file_prefix + \"_AugmentedManifestFile.jsonl\"\n",
    "    if exists(filename):\n",
    "        print(f\"{filename} exists, removing now\")\n",
    "        !rm $filename\n",
    "        \n",
    "    # Writes the JSON objects to a JSON Lines file.\n",
    "    with open(f\"{file_prefix}_AugmentedManifestFile.jsonl\", \"a\") as f:\n",
    "        for obj in all_json_objects:\n",
    "            text = str(json.loads(obj))\n",
    "            f.write(text)\n",
    "            f.write('\\n')"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "98555c3e",
   "metadata": {},
   "source": [
    "This function removes all spectrograms from the data folder."
   ]
  },
  {
   "cell_type": "code",
   "id": "e054a8a8",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-30T19:41:15.647007Z",
     "start_time": "2024-05-30T19:41:15.645046Z"
    }
   },
   "source": [
    "def cleanup():\n",
    "    !rm data/*"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "19116d91",
   "metadata": {},
   "source": [
    "This function removes an old LST file from the working directory when a new one is being created to replace it."
   ]
  },
  {
   "cell_type": "code",
   "id": "4c2a3bca",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-30T19:41:19.113332Z",
     "start_time": "2024-05-30T19:41:19.111074Z"
    }
   },
   "source": [
    "def remove_Lst_fileIfOpen(file_name):\n",
    "    \"\"\"\n",
    "    Removes a LST file from the working directory with the same name as the one being created (if one was created previously).\n",
    "    \"\"\"\n",
    "    if exists(file_name):\n",
    "        print(f\"{file_name} exists, removing now\")\n",
    "        !rm $file_name"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "96574ffd",
   "metadata": {},
   "source": [
    "The \"copy_to_bucket()\" function can copy any file from this notebook's working directory to the S3 Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c794151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def copy_to_bucket(fileSource, fileDestination):\n",
    "#     \"\"\"\n",
    "#     Copies a file from this notebook's working directory to the S3 Bucket.\n",
    "#     \"\"\"\n",
    "#     # NOTE: Change the following name of the S3 Bucket (in parentheses) to reflect the name of the S3 Bucket for your current AWS account.\n",
    "#     \"\"\"WARNING: This S3 Bucket should be the one that contains SageMaker files (NOT the one with WAV files and TXT files).\"\"\"\n",
    "#     write_bucket = s3.Bucket('sagemaker-us-...')\n",
    "#     write_bucket.upload_file(fileSource, fileDestination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de45f10",
   "metadata": {},
   "source": [
    "This function takes in a list of WAV file names (along with the desired name for the final data file). \n",
    "It then creates the corresponding spectrograms, LST file, and final data file.\n",
    "Depending on the \"file_format\", the final data file will either be a REC file or an Augmented Manifest file."
   ]
  },
  {
   "cell_type": "code",
   "id": "4c14d68f",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-30T19:41:23.112194Z",
     "start_time": "2024-05-30T19:41:23.109478Z"
    }
   },
   "source": [
    "def create_data_files(dataset, file_prefix, file_format = file_format):\n",
    "    \"\"\"\n",
    "    This function takes in a list of WAV file names, and the name of the REC file the annotations need to be stored in, \n",
    "    and then creates the corresponding spectrograms, LST file, and REC file for the WAV files.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "        dataset: string\n",
    "            Specifies which dataset (train_set, validation_set, or testing_set) to produce a LST file and final data file for.\n",
    "        file_prefix: string\n",
    "            Specifies the name that you want the LST file and final data file to have (without the \".lst\", \".rec\", or \".jsonl\" portions).\n",
    "                (Corresponds to the LST file created at the same time as \"aug_manif_info\".)\n",
    "        file_format: string\n",
    "            Specifies the desired format for the final data file (which would be used during any training jobs and tuning jobs).\n",
    "                (Either \"REC\" or \"AugmentedManifest\")\n",
    "    ----------\n",
    "    \n",
    "    RETURNS\n",
    "    ----------\n",
    "        N/A\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    # Produces spectrograms and LST file\n",
    "    lst_file_name = f\"{file_prefix}.lst\"\n",
    "    remove_Lst_fileIfOpen(lst_file_name)\n",
    "    aug_manif_info = create_lst_file(dataset, lst_file_name)\n",
    "    \n",
    "    # Produces either a REC file or Augmented Manifest file (depending on file_format)\n",
    "    if file_format == \"REC\":\n",
    "        create_rec_file(lst_file_name)\n",
    "    elif file_format == \"AugmentedManifest\":\n",
    "        create_augmented_manifest_file(file_prefix, aug_manif_info)\n",
    "        \n",
    "    # Displays message indicating that all desired files have been created\n",
    "    print(\"Done!\")"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "e4a0afba",
   "metadata": {},
   "source": [
    "Remember to call cleanup() before any call to create_data_files()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8db2950",
   "metadata": {},
   "source": [
    "## Getting Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb0054",
   "metadata": {},
   "source": [
    "Deletes all spectrograms in the \"data\" directory."
   ]
  },
  {
   "cell_type": "code",
   "id": "92311406",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-30T19:44:12.526887Z",
     "start_time": "2024-05-30T19:44:12.405511Z"
    }
   },
   "source": [
    "cleanup()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: data/*\r\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "642c6a7e",
   "metadata": {},
   "source": "|Creates spectrograms, LST file, and final data file for the training data."
  },
  {
   "cell_type": "code",
   "id": "2cef05fc",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-05-30T21:19:31.446502Z",
     "start_time": "2024-05-30T21:19:31.394835Z"
    }
   },
   "source": [
    "create_data_files(train_set, \"train_full\", file_format = file_format)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/31 wav files converted\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '6805.230207120827_processed.wav_processed.wav'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m create_data_files(train_set, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_full\u001B[39m\u001B[38;5;124m\"\u001B[39m, file_format \u001B[38;5;241m=\u001B[39m file_format)\n",
      "Cell \u001B[0;32mIn[20], line 26\u001B[0m, in \u001B[0;36mcreate_data_files\u001B[0;34m(dataset, file_prefix, file_format)\u001B[0m\n\u001B[1;32m     24\u001B[0m lst_file_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.lst\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     25\u001B[0m remove_Lst_fileIfOpen(lst_file_name)\n\u001B[0;32m---> 26\u001B[0m aug_manif_info \u001B[38;5;241m=\u001B[39m create_lst_file(dataset, lst_file_name)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Produces either a REC file or Augmented Manifest file (depending on file_format)\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file_format \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mREC\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "Cell \u001B[0;32mIn[14], line 37\u001B[0m, in \u001B[0;36mcreate_lst_file\u001B[0;34m(dataset, lst_file_name)\u001B[0m\n\u001B[1;32m     34\u001B[0m index \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Produces the spectrograms and LST file lines corresponding to the current WAV file\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m cur_info \u001B[38;5;241m=\u001B[39m process_file(file, file, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m80.0\u001B[39m, \u001B[38;5;241m0\u001B[39m, CHUNK_SIZE_SEC, lst_file_name,chunk_layout\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdense\u001B[39m\u001B[38;5;124m\"\u001B[39m, drop_last_chunk\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# Appends important spectrogram information and annotation information to the list\u001B[39;00m\n\u001B[1;32m     39\u001B[0m aug_manif_info\u001B[38;5;241m.\u001B[39mappend(cur_info)\n",
      "Cell \u001B[0;32mIn[13], line 42\u001B[0m, in \u001B[0;36mprocess_file\u001B[0;34m(wav_filename, annot_filename, min_bound, max_bound, chunk_size, lst_file_name, chunk_layout, drop_last_chunk, verbose)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03mIterates through all data chunks in the current WAV file, serving as a shell for extract_chunk().\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m----------\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Reads-in WAV file information and annotation information\u001B[39;00m\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;66;03m# For every decimated WAV file, sampling rate (sr) seems to be 8000 samples per second.\u001B[39;00m\n\u001B[0;32m---> 42\u001B[0m sr, data \u001B[38;5;241m=\u001B[39m read_wavfile(wav_filename, normalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, verbose\u001B[38;5;241m=\u001B[39mverbose)\n\u001B[1;32m     43\u001B[0m annotations \u001B[38;5;241m=\u001B[39m read_annotations(annot_filename, verbose\u001B[38;5;241m=\u001B[39mverbose)\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# Converts spectrogram constants from being measured in seconds to being measured in samples\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[32], line 31\u001B[0m, in \u001B[0;36mread_wavfile\u001B[0;34m(wav_name, normalize, verbose)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose:\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReading \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(file_name))     \n\u001B[0;32m---> 31\u001B[0m sr, data \u001B[38;5;241m=\u001B[39m wavfile\u001B[38;5;241m.\u001B[39mread(file_name)\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;66;03m# NOTE: Sampling rate (sr) seems to be 8000 samples per second\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(sr)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/scipy/io/wavfile.py:647\u001B[0m, in \u001B[0;36mread\u001B[0;34m(filename, mmap)\u001B[0m\n\u001B[1;32m    645\u001B[0m     mmap \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    646\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 647\u001B[0m     fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(filename, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    649\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    650\u001B[0m     file_size, is_big_endian \u001B[38;5;241m=\u001B[39m _read_riff_chunk(fid)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '6805.230207120827_processed.wav_processed.wav'"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "95cad480",
   "metadata": {},
   "source": [
    "Copies training REC file from here to the S3 Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d7c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"WARNING: I recommend archiving the train_full.rec file currently in the S3 Bucket before replacing it with the new one.\"\"\"\n",
    "#copy_to_bucket(\"train_full.rec\", \"train/train_full.rec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b075c5a",
   "metadata": {},
   "source": [
    "Copies training Augmented Manifest File to the S3 Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b40ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WARNING: I recommend archiving the train_full_AugmentedManifestFile.jsonl file currently in the S3 Bucket before replacing it with the new one.\"\"\"\n",
    "#copy_to_bucket(\"train_full_AugmentedManifestFile.jsonl\", \"train_full_manifest/train_full_AugmentedManifestFile.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d3b74f",
   "metadata": {},
   "source": [
    "Copies training spectrograms to the correct location in the S3 Bucket (specified in the Augmented Manifest File). Note that, in this case, the spectrograms should be produced in the professor's S3 Bucket, so that they can be easily copied over for model training."
   ]
  },
  {
   "cell_type": "code",
   "id": "249aeae3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:08:21.496025Z",
     "start_time": "2024-05-21T21:08:21.493594Z"
    }
   },
   "source": [
    "import glob\n",
    "\n",
    "# Gets the file path to each PNG file in the \"data/\" directory\n",
    "files = glob.glob('data/*.png')\n",
    "\n",
    "# Iterates through all spectrograms in \"files\", copying each one to the S3 Bucket\n",
    "for file in files:\n",
    "    spec_name = file.split(\"/\")[-1]\n",
    "    copy_to_bucket(file, \"train_full_manifest/spectrograms/\" + spec_name)"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "eb083a0d",
   "metadata": {},
   "source": [
    "## Getting Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade3d05",
   "metadata": {},
   "source": [
    "Deletes all spectrograms in the \"data\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d19869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a483f20",
   "metadata": {},
   "source": [
    "Creates spectrograms, LST file, and final data file for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b2d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_data_files(validation_set, \"val\", file_format = file_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470afe0",
   "metadata": {},
   "source": [
    "Copies validation REC file from here to the S3 Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be39572f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"WARNING: I recommend archiving the val.rec file currently in the S3 Bucket before replacing it with the new one.\"\"\"\n",
    "#copy_to_bucket(\"val.rec\", \"validation/val.rec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a690d",
   "metadata": {},
   "source": [
    "Copies validation Augmented Manifest File to the S3 Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f7d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WARNING: I recommend archiving the val_AugmentedManifestFile.jsonl file currently in the S3 Bucket before replacing it with the new one.\"\"\"\n",
    "#copy_to_bucket(\"val_AugmentedManifestFile.jsonl\", \"val_manifest/val_AugmentedManifestFile.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8700632",
   "metadata": {},
   "source": [
    "Copies validation spectrograms to the correct location in the S3 Bucket (specified in the Augmented Manifest File). Note that, in this case, the spectrograms should be produced in the professor's S3 Bucket, so that they can be easily copied over for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28fd6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Gets the file path to each PNG file in the \"data/\" directory\n",
    "files = glob.glob('data/*.png')\n",
    "\n",
    "# Iterates through all spectrograms in \"files\", copying each one to the S3 Bucket\n",
    "for file in files:\n",
    "    spec_name = file.split(\"/\")[-1]\n",
    "    copy_to_bucket(file, \"val_manifest/spectrograms/\" + spec_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f35849",
   "metadata": {},
   "source": [
    "## Getting Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e615a9",
   "metadata": {},
   "source": [
    "Deletes all spectrograms in the \"data\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168544e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa980b4",
   "metadata": {},
   "source": [
    "Creates spectrograms, LST file, and final data file for the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data_files(testing_set, \"test\", file_format = file_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16df5ea",
   "metadata": {},
   "source": [
    "Copies testing REC file from here to the S3 Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WARNING: I recommend archiving the test.rec file currently in the S3 Bucket before replacing it with the new one.\"\"\"\n",
    "#copy_to_bucket(\"test.rec\", \"testing/test.rec\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
