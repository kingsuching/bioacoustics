{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d91f6c112f0ac7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T23:45:32.134914Z",
     "start_time": "2024-02-23T23:45:32.120294Z"
    }
   },
   "outputs": [],
   "source": [
    "# Only run once\n",
    "# !pip install librosa\n",
    "#!pip install tensorflow_probability\n",
    "#!pip install tensorflow_addons\n",
    "#!pip install scikit-maad"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Functions\n",
    "\n",
    "This notebook is called by the vae notebook. Do not run this notebook separately."
   ],
   "id": "dd83efc1e907f67d"
  },
  {
   "cell_type": "code",
   "id": "976a2749aa92754c",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/sucheen/anaconda3/lib/python3.11/site-packages\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras import layers,Input\n",
    "from tensorflow.keras.layers import Dense,Lambda\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from PIL import ImageColor,ImageFont\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import pdb\n",
    "import glob\n",
    "import cv2\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import tensorflow_addons as tfa\n",
    "import boto3\n",
    "\n",
    "%run preprocessing.ipynb\n",
    "KEYS = \"ssundar_accessKeys.csv\"\n",
    "aws_access_key_id, aws_secret_access_key = awsKeys(KEYS)\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name='us-east-1'  # or your preferred region\n",
    ")\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "# S3 Bucket for Professor's Account is 'monitoring-whale-recordings'\n",
    "# S3 Bucket for our free tier Account is 'monitoring-whale-records'\n",
    "bucket_name = 'whale-recordings'\n",
    "bucket = s3.Bucket(bucket_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c445dbabb803046a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T23:49:21.274359Z",
     "start_time": "2024-02-23T23:49:21.245837Z"
    }
   },
   "outputs": [],
   "source": [
    "class AudioVAE(keras.Model):\n",
    "    def __init__(self, latent_dim, sr, num_heads=4, key_dim=64, value_dim=64):\n",
    "        super(AudioVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sr = sr\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.value_dim = value_dim\n",
    "        \n",
    "        \n",
    "        self.encoder = keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(608,192, 1)),\n",
    "            layers.Conv2D(filters=32, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2D(filters=64, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2D(filters=128, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2D(filters=256, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2D(filters=512, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(latent_dim)\n",
    "        ])\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(int(latent_dim/2),)),\n",
    "            layers.Dense(units=19*6*512, activation='relu'),\n",
    "            layers.Reshape(target_shape=(19, 6, 512)),\n",
    "            layers.Conv2DTranspose(filters=512, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2DTranspose(filters=256, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2DTranspose(filters=128, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2DTranspose(filters=32, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2DTranspose(filters=1, kernel_size=5, strides=1, padding='same', activation='linear')\n",
    "         ])        \n",
    "\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Encode input\n",
    "            reconstruction,mean,logvar=self(x)\n",
    "            loss = vae_loss_function(x, reconstruction, mean, logvar)\n",
    "            reconstruction_loss = loss[\"reconstruction_loss\"]\n",
    "            kl_loss = loss[\"kl_loss\"]\n",
    "            total_loss=loss[\"total_loss\"]\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\"kl_loss\": kl_loss,\"reconstruction_loss\":reconstruction_loss,\"total_loss\":total_loss}\n",
    "        \n",
    "    @tf.function\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    @tf.function\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        batch_size = tf.shape(mean)[0]\n",
    "        latent_dim = tf.shape(mean)[1]\n",
    "        eps = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "        return eps * tf.exp(logvar * 0.5) + mean\n",
    "    \n",
    "    @tf.function\n",
    "    def decode(self, z):\n",
    "        recon = self.decoder(z)\n",
    "        return recon\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        reconstruction = self.decode(z)\n",
    "        return reconstruction, mean, logvar\n",
    "\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def reconstructed_probability(self, x, mean, logvar):\n",
    "        \n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float64)\n",
    "        x = tf.cast(x, dtype=tf.float32)\n",
    "        recon_dist = tfd.Normal(loc=mean, scale=tf.math.exp(0.5*logvar))\n",
    "        x = tf.expand_dims(x, 0)\n",
    "        p = tf.exp(recon_dist.log_prob(x).mean(axis=0).mean(axis=-1))  # vector of shape [batch_size]\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7093fe60513a543b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T23:49:21.940926Z",
     "start_time": "2024-02-23T23:49:21.926202Z"
    }
   },
   "outputs": [],
   "source": [
    "def vae_loss_function(x, reconstruction, mean, logvar,prediction=False):\n",
    "    reconstruction_loss=tf.keras.losses.MeanAbsoluteError()(x,reconstruction)\n",
    "    reconstruction_loss = tf.reduce_mean(reconstruction_loss)\n",
    "    # Compute KL divergence loss\n",
    "    kl_loss = 1 + logvar - tf.square(mean) - tf.exp(logvar)\n",
    "    kl_loss = -0.5 * tf.reduce_sum(kl_loss, axis=-1)\n",
    "    \n",
    "\n",
    "    \n",
    "    if prediction:\n",
    "        return {\"reconstruction_loss\": reconstruction_loss, \"kl_loss\": kl_loss} \n",
    "    \n",
    "    # Reduce the losses to a scalar\n",
    "    kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "    return {\"reconstruction_loss\": reconstruction_loss, \"kl_loss\": kl_loss, \"total_loss\":reconstruction_loss+kl_loss}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c6091651cfbef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T23:49:22.221454Z",
     "start_time": "2024-02-23T23:49:22.209477Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(latent_dim,sr):\n",
    "    # Initialize VAE\n",
    "    vae = AudioVAE(latent_dim,sr)\n",
    "\n",
    "\n",
    "    # Compile VAE\n",
    "    optimizer = tfa.optimizers.AdaBelief(lr=0.0006)\n",
    "    vae.compile(optimizer=optimizer, loss=vae_loss_function)\n",
    "\n",
    "\n",
    "    vae.encoder.summary()\n",
    "    vae.decoder.summary()\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "393587ebc0d152ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T23:49:22.628478Z",
     "start_time": "2024-02-23T23:49:22.613921Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_confidence(heatmap, contour,max_value):\n",
    "    # Get the region of interest from the heatmap based on the contour\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    roi = heatmap[y:y+h, x:x+w]\n",
    "    \n",
    "    # Calculate the average pixel intensity of the region of interest\n",
    "    average_intensity = np.mean(roi)\n",
    "    \n",
    "    # Calculate the confidence value by dividing the average intensity by the maximum known intensity (255)\n",
    "    confidence = average_intensity / max_value\n",
    "    \n",
    "    return confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcdb6429f8e70bd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:28:30.315763Z",
     "start_time": "2024-04-26T21:28:30.305276Z"
    }
   },
   "outputs": [],
   "source": [
    "from maad import sound, rois, features\n",
    "from maad.util import (power2dB, plot2d, format_features, read_audacity_annot,\n",
    "                       overlay_rois, overlay_centroid)\n",
    "def apply_bounding_boxes(spectrogram,running=True,time_reference=0.0):\n",
    "    sr = 8000\n",
    "    \n",
    "    \n",
    "    \n",
    "    WINDOW_SIZE_SEC = 0.15175\n",
    "    HOP_LEN_SEC = 0.05\n",
    "    # Reads-in WAV file information (and annotation information)\n",
    "\n",
    "    # Parameters needed for the stream\n",
    "    n_fft = int(WINDOW_SIZE_SEC * sr)\n",
    "    hop_length = int(HOP_LEN_SEC * sr)\n",
    "    \n",
    "    # Compute the frequency values in Hz\n",
    "    frequencies = librosa.core.fft_frequencies(sr=sr, n_fft=n_fft)\n",
    "    times = np.arange(0,9.6,0.05)\n",
    "    \n",
    "    \n",
    "    #GRAPH ATTEMPT\n",
    "    # Convert the spectrogram to an 8-bit grayscale image\n",
    "    spectrogram_gray = (spectrogram * 255).astype(np.uint8)\n",
    "    spectrogram_rgb = cv2.cvtColor(spectrogram_gray, cv2.COLOR_GRAY2RGB)\n",
    "    n=0\n",
    "    img=spectrogram_rgb\n",
    "    \n",
    "    \n",
    "#     Sxx_power_noNoise= sound.median_equalizer(spectrogram, display=True)\n",
    "#     Sxx_db_noNoise = power2dB(spectrogram)\n",
    "\n",
    "    # Then we smooth the spectrogram in order to facilitate the creation of masks as\n",
    "    # small sparse details are merged if they are close to each other\n",
    "    Sxx_db_noNoise_smooth = sound.smooth(spectrogram, std=0.5,\n",
    "                             display=False, savefig=None)\n",
    "\n",
    "    # Then we create a mask (i.e. binarization of the spectrogram) by using the\n",
    "    # double thresholding technique\n",
    "    im_mask = rois.create_mask(im=Sxx_db_noNoise_smooth, mode_bin ='relative',\n",
    "                               bin_std=16, bin_per=0.5,\n",
    "                               verbose=False, display=False)\n",
    "\n",
    "    # Finaly, we put together pixels that belong to the same acoustic event, and\n",
    "    # remove very small events (<=25 pixelÂ²)\n",
    "    im_rois, df_rois = rois.select_rois(im_mask, min_roi=30, max_roi=None,\n",
    "                                     display= False)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    rects=df_rois[[\"min_x\",\"max_x\",\"min_y\",\"max_y\"]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    boxes=[]\n",
    "    # Iterate over the resulting bounding boxes\n",
    "    for box in rects.iterrows():\n",
    "        x1,x2,y1,y2=box[1]\n",
    "        if times[x1-1]<times[x2-1] and frequencies[y1-1]<frequencies[y2-1]:\n",
    "            roi = spectrogram[y1-1:y2, x1-1:x2]\n",
    "            # Calculate the average pixel intensity of the region of interest\n",
    "            average_intensity = np.mean(roi)\n",
    "\n",
    "            # Calculate the confidence value by dividing the average intensity by the maximum known intensity (255)\n",
    "            max_compar = np.max(spectrogram)\n",
    "            confidence = average_intensity / max_compar\n",
    "            if running:\n",
    "                row=[times[x1-1]+time_reference,times[x2-1]+time_reference,frequencies[y1-1],frequencies[y2-1],confidence]\n",
    "                boxes.append(row)\n",
    "            else:\n",
    "                boxes.append([time_constant*x,time_constant*(x+w),frequency_constant*y,frequency_constant*(y+h),confidence])\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19310d4cc8d1ba6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T23:49:26.021636Z",
     "start_time": "2024-02-23T23:49:25.999286Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(train_dataset,save=True):\n",
    "    segments=1\n",
    "    latent_dim=2000\n",
    "    vae=build_model(latent_dim,None)\n",
    "    vae.fit(train_dataset, epochs=5)\n",
    "    if save:\n",
    "        # Save model weights to a file\n",
    "        vae.save(\"test_vae_mod_pcen\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "131c98e6fce6f5e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T23:49:26.437759Z",
     "start_time": "2024-02-23T23:49:26.418741Z"
    }
   },
   "outputs": [],
   "source": [
    "def error_dataset(vae,data,full=True,sr=None):\n",
    "    reconstruction,_,_=vae.predict(data)\n",
    "    reconstruction_loss=tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)(data,reconstruction).numpy()\n",
    "    return reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c0b93e83de778f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T23:49:26.793452Z",
     "start_time": "2024-02-23T23:49:26.779465Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_array_to_file(array, headers, filename):\n",
    "    # Open the file for writing\n",
    "    with open(filename, 'w') as file:\n",
    "        # Write the headers to the file\n",
    "        header_line = '\\t'.join(headers)  # Join headers with tabs\n",
    "        file.write(header_line + '\\n')\n",
    "\n",
    "        # Write the array data to the file\n",
    "        for row in array:\n",
    "            row_line = '\\t'.join(map(str, row))  # Join row elements with tabs\n",
    "            file.write(row_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76e59f1efc05578c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T23:49:27.202781Z",
     "start_time": "2024-02-23T23:49:27.185947Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model(non_normal_scores):\n",
    "    bounding_boxes=[]\n",
    "    reference=0.0\n",
    "    \n",
    "    for i in non_normal_scores:\n",
    "        bounding_boxes+=apply_bounding_boxes(i,True,reference)\n",
    "        reference+=9.6\n",
    "#     print(\"DONE\")\n",
    "    bounding_boxes=np.array(bounding_boxes)\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c37b5bf7a839e203",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T23:04:29.327200Z",
     "start_time": "2024-02-26T23:04:29.318641Z"
    }
   },
   "outputs": [],
   "source": [
    "def exclude(audio_file, labels_file):\n",
    "    # Load audio file\n",
    "    y, sr = sf.read(audio_file,dtype=\"float32\")\n",
    "\n",
    "    # Load labels file as pandas dataframe\n",
    "    df = pd.read_csv(labels_file, delimiter='\\t')\n",
    "\n",
    "    # Convert start and end times to sample indices\n",
    "    start_idx = librosa.time_to_samples(df['Begin Time (s)'], sr=sr)\n",
    "    end_idx = librosa.time_to_samples(df['End Time (s)'], sr=sr)\n",
    "\n",
    "    # Create a boolean mask for each frame\n",
    "    frames = librosa.util.frame(y, frame_length=sr, hop_length=sr).T\n",
    "    mask = np.ones(frames.shape[0], dtype=bool)\n",
    "\n",
    "    # Loop over each interval and exclude corresponding frames\n",
    "    for idx in range(len(start_idx)):\n",
    "        start_frame = start_idx[idx] // sr\n",
    "        end_frame = end_idx[idx] // sr\n",
    "        mask[start_frame:end_frame+1] = False\n",
    "\n",
    "    # Apply mask to frames\n",
    "    frames_filtered = frames[mask]\n",
    "\n",
    "    # Reshape filtered frames into audio signal\n",
    "    y_filtered = frames_filtered.reshape(-1)\n",
    "    \n",
    "    filtered_filename = f'{audio_file[:-4]}_filtered.wav'\n",
    "    sf.write(filtered_filename, y_filtered, sr)\n",
    "    return sr, y_filtered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
