{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2641d385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T22:31:21.192190Z",
     "start_time": "2024-02-26T22:31:21.162392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Only run once\n",
    "# !pip install librosa\n",
    "#!pip install tensorflow_probability\n",
    "#!pip install tensorflow_addons\n",
    "#!pip install scikit-maad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58913452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T22:34:33.339129Z",
     "start_time": "2024-02-26T22:34:22.505210Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sucheen/anaconda3/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished preprocessing\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/sucheen/anaconda3/lib/python3.11/site-packages')\n",
    "from tensorflow.keras import layers,Input\n",
    "from tensorflow.keras.layers import Dense,Lambda\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from PIL import ImageColor,ImageFont\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import pdb\n",
    "import glob\n",
    "import cv2\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import tensorflow_addons as tfa\n",
    "import boto3\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%run preprocessing.ipynb\n",
    "KEYS = \"../../ssundar_accessKeys.csv\"\n",
    "aws_access_key_id, aws_secret_access_key = awsKeys(KEYS)\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name='us-east-1'  # or your preferred region\n",
    ")\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "# S3 Bucket for Professor's Account is 'monitoring-whale-recordings'\n",
    "# S3 Bucket for our free tier Account is 'monitoring-whale-records'\n",
    "bucket_name = 'monitoring-whale-recordings'\n",
    "bucket = s3.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "346aa1ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T22:34:46.908923Z",
     "start_time": "2024-02-26T22:34:46.893057Z"
    }
   },
   "outputs": [],
   "source": [
    "class AudioVAE(keras.Model):\n",
    "    def __init__(self, latent_dim, sr, num_heads=4, key_dim=64, value_dim=64):\n",
    "        super(AudioVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sr = sr\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.value_dim = value_dim\n",
    "        \n",
    "        \n",
    "        self.encoder = keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(608,192, 1)),\n",
    "            layers.Conv2D(filters=32, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2D(filters=64, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2D(filters=128, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2D(filters=256, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2D(filters=512, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(latent_dim)\n",
    "        ])\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(int(latent_dim/2),)),\n",
    "            layers.Dense(units=19*6*512, activation='relu'),\n",
    "            layers.Reshape(target_shape=(19, 6, 512)),\n",
    "            layers.Conv2DTranspose(filters=512, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2DTranspose(filters=256, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2DTranspose(filters=128, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2DTranspose(filters=32, kernel_size=5, strides=2, padding='same'),\n",
    "            tf.keras.layers.Lambda(lambda x: tfa.activations.gelu(x)),\n",
    "            layers.Conv2DTranspose(filters=1, kernel_size=5, strides=1, padding='same', activation='linear')\n",
    "         ])        \n",
    "\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Encode input\n",
    "            reconstruction,mean,logvar=self(x)\n",
    "            loss = vae_loss_function(x, reconstruction, mean, logvar)\n",
    "            reconstruction_loss = loss[\"reconstruction_loss\"]\n",
    "            kl_loss = loss[\"kl_loss\"]\n",
    "            total_loss=loss[\"total_loss\"]\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\"kl_loss\": kl_loss,\"reconstruction_loss\":reconstruction_loss,\"total_loss\":total_loss}\n",
    "        \n",
    "    @tf.function\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    @tf.function\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        batch_size = tf.shape(mean)[0]\n",
    "        latent_dim = tf.shape(mean)[1]\n",
    "        eps = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "        return eps * tf.exp(logvar * 0.5) + mean\n",
    "    \n",
    "    @tf.function\n",
    "    def decode(self, z):\n",
    "        recon = self.decoder(z)\n",
    "        return recon\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        reconstruction = self.decode(z)\n",
    "        return reconstruction, mean, logvar\n",
    "\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def reconstructed_probability(self, x, mean, logvar):\n",
    "        \n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float64)\n",
    "        x = tf.cast(x, dtype=tf.float32)\n",
    "        recon_dist = tfd.Normal(loc=mean, scale=tf.math.exp(0.5*logvar))\n",
    "        x = tf.expand_dims(x, 0)\n",
    "        p = tf.exp(recon_dist.log_prob(x).mean(axis=0).mean(axis=-1))  # vector of shape [batch_size]\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06c54b21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T22:34:47.217293Z",
     "start_time": "2024-02-26T22:34:47.200098Z"
    }
   },
   "outputs": [],
   "source": [
    "def vae_loss_function(x, reconstruction, mean, logvar,prediction=False):\n",
    "    reconstruction_loss=tf.keras.losses.MeanAbsoluteError()(x,reconstruction)\n",
    "    reconstruction_loss = tf.reduce_mean(reconstruction_loss)\n",
    "    # Compute KL divergence loss\n",
    "    kl_loss = 1 + logvar - tf.square(mean) - tf.exp(logvar)\n",
    "    kl_loss = -0.5 * tf.reduce_sum(kl_loss, axis=-1)\n",
    "    \n",
    "\n",
    "    \n",
    "    if prediction:\n",
    "        return {\"reconstruction_loss\": reconstruction_loss, \"kl_loss\": kl_loss} \n",
    "    \n",
    "    # Reduce the losses to a scalar\n",
    "    kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "    return {\"reconstruction_loss\": reconstruction_loss, \"kl_loss\": kl_loss, \"total_loss\":reconstruction_loss+kl_loss}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09818aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T22:34:47.379769Z",
     "start_time": "2024-02-26T22:34:47.357974Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(latent_dim,sr):\n",
    "    # Initialize VAE\n",
    "    vae = AudioVAE(latent_dim,sr)\n",
    "\n",
    "\n",
    "    # Compile VAE\n",
    "    optimizer = tfa.optimizers.AdaBelief(lr=0.0006)\n",
    "    vae.compile(optimizer=optimizer, loss=vae_loss_function)\n",
    "\n",
    "\n",
    "    vae.encoder.summary()\n",
    "    vae.decoder.summary()\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb4aec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T22:34:47.526539Z",
     "start_time": "2024-02-26T22:34:47.508369Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_confidence(heatmap, contour,max_value):\n",
    "    # Get the region of interest from the heatmap based on the contour\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    roi = heatmap[y:y+h, x:x+w]\n",
    "    \n",
    "    # Calculate the average pixel intensity of the region of interest\n",
    "    average_intensity = np.mean(roi)\n",
    "    \n",
    "    # Calculate the confidence value by dividing the average intensity by the maximum known intensity (255)\n",
    "    confidence = average_intensity / max_value\n",
    "    \n",
    "    return confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dea2e04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T22:34:49.140296Z",
     "start_time": "2024-02-26T22:34:47.668819Z"
    }
   },
   "outputs": [],
   "source": [
    "from maad import sound, rois, features\n",
    "from maad.util import (power2dB, plot2d, format_features, read_audacity_annot,\n",
    "                       overlay_rois, overlay_centroid)\n",
    "def apply_bounding_boxes(spectrogram,running=True,time_reference=0.0):\n",
    "    sr = 8000\n",
    "    \n",
    "    \n",
    "    \n",
    "    WINDOW_SIZE_SEC = 0.15175\n",
    "    HOP_LEN_SEC = 0.05\n",
    "    # Reads-in WAV file information (and annotation information)\n",
    "\n",
    "    # Parameters needed for the stream\n",
    "    n_fft = int(WINDOW_SIZE_SEC * sr)\n",
    "    hop_length = int(HOP_LEN_SEC * sr)\n",
    "    \n",
    "    # Compute the frequency values in Hz\n",
    "    frequencies = librosa.core.fft_frequencies(sr=sr, n_fft=n_fft)\n",
    "    times = np.arange(0,9.6,0.05)\n",
    "    \n",
    "    \n",
    "    #GRAPH ATTEMPT\n",
    "    # Convert the spectrogram to an 8-bit grayscale image\n",
    "    spectrogram_gray = (spectrogram * 255).astype(np.uint8)\n",
    "    spectrogram_rgb = cv2.cvtColor(spectrogram_gray, cv2.COLOR_GRAY2RGB)\n",
    "    n=0\n",
    "    img=spectrogram_rgb\n",
    "    \n",
    "    \n",
    "#     Sxx_power_noNoise= sound.median_equalizer(spectrogram, display=True)\n",
    "#     Sxx_db_noNoise = power2dB(spectrogram)\n",
    "\n",
    "    # Then we smooth the spectrogram in order to facilitate the creation of masks as\n",
    "    # small sparse details are merged if they are close to each other\n",
    "    Sxx_db_noNoise_smooth = sound.smooth(spectrogram, std=0.5,\n",
    "                             display=False, savefig=None)\n",
    "\n",
    "    # Then we create a mask (i.e. binarization of the spectrogram) by using the\n",
    "    # double thresholding technique\n",
    "    im_mask = rois.create_mask(im=Sxx_db_noNoise_smooth, mode_bin ='relative',\n",
    "                               bin_std=16, bin_per=0.5,\n",
    "                               verbose=False, display=False)\n",
    "\n",
    "    # Finaly, we put together pixels that belong to the same acoustic event, and\n",
    "    # remove very small events (<=25 pixelÂ²)\n",
    "    im_rois, df_rois = rois.select_rois(im_mask, min_roi=30, max_roi=None,\n",
    "                                     display= False)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    rects=df_rois[[\"min_x\",\"max_x\",\"min_y\",\"max_y\"]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    boxes=[]\n",
    "    # Iterate over the resulting bounding boxes\n",
    "    for box in rects.iterrows():\n",
    "        x1,x2,y1,y2=box[1]\n",
    "        if times[x1-1]<times[x2-1] and frequencies[y1-1]<frequencies[y2-1]:\n",
    "            roi = spectrogram[y1-1:y2, x1-1:x2]\n",
    "            # Calculate the average pixel intensity of the region of interest\n",
    "            average_intensity = np.mean(roi)\n",
    "\n",
    "            # Calculate the confidence value by dividing the average intensity by the maximum known intensity (255)\n",
    "            # max_compar=np.quantile(spectrogram,0.99)\n",
    "            max_compar = np.max(spectrogram)\n",
    "            confidence = average_intensity / max_compar\n",
    "            \n",
    "            if running:\n",
    "                row=[times[x1-1]+time_reference,times[x2-1]+time_reference,frequencies[y1-1],frequencies[y2-1],confidence]\n",
    "                boxes.append(row)\n",
    "            else:\n",
    "                boxes.append([time_constant*x,time_constant*(x+w),frequency_constant*y,frequency_constant*(y+h),confidence])\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cffb0aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T22:34:50.466073Z",
     "start_time": "2024-02-26T22:34:50.439379Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(train_dataset,save=True):\n",
    "    segments=1\n",
    "    latent_dim=2000\n",
    "    vae=build_model(latent_dim,None)\n",
    "    vae.fit(train_dataset, epochs=5)\n",
    "    if save:\n",
    "        # Save model weights to a file\n",
    "        vae.save(\"test_vae_mod_pcen\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10736736",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T22:34:50.799491Z",
     "start_time": "2024-02-26T22:34:50.774781Z"
    }
   },
   "outputs": [],
   "source": [
    "def error_dataset(vae,data,full=True,sr=None):\n",
    "    reconstruction,_,_=vae.predict(data)\n",
    "    reconstruction_loss=tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)(data,reconstruction).numpy()\n",
    "    return reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3733e69b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T22:34:51.319652Z",
     "start_time": "2024-02-26T22:34:51.297047Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_array_to_file(array, headers, filename):\n",
    "    # Open the file for writing\n",
    "    with open(filename, 'w') as file:\n",
    "        # Write the headers to the file\n",
    "        header_line = '\\t'.join(headers)  # Join headers with tabs\n",
    "        file.write(header_line + '\\n')\n",
    "\n",
    "        # Write the array data to the file\n",
    "        for row in array:\n",
    "            row_line = '\\t'.join(map(str, row))  # Join row elements with tabs\n",
    "            file.write(row_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "827a26ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T22:34:51.957418Z",
     "start_time": "2024-02-26T22:34:51.921897Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model(non_normal_scores):\n",
    "    bounding_boxes=[]\n",
    "    reference=0.0\n",
    "    \n",
    "    for i in non_normal_scores:\n",
    "        bounding_boxes+=apply_bounding_boxes(i,True,reference)\n",
    "        reference+=9.6\n",
    "#     print(\"DONE\")\n",
    "    bounding_boxes=np.array(bounding_boxes)\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd957b53",
   "metadata": {},
   "source": [
    "# Code to train and get initial bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0937910d",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T22:35:09.756696Z",
     "start_time": "2024-02-26T22:34:58.529543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6805.230201070825_processed.wav\n",
      "training\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "background_noise = \"avila_filtered.wav\"\n",
    "predicting_file = \"6805.230201070825_processed.wav\"\n",
    "print(predicting_file)\n",
    "\n",
    "# Get predicting file\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "bucket_name = 'whale-recordings'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "for wf in WAV_FILES:\n",
    "    if predicting_file == wf[0]:\n",
    "        # Download WAV file from S3 Bucket\n",
    "        s3_client.download_file(bucket_name, wf[1], wf[0])\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"training\")\n",
    "dataset_train,sr = process_wav(background_noise)\n",
    "#sys.exit(0)\n",
    "print(\"testing\")\n",
    "dataset_test,sr = process_wav(predicting_file, running = True)\n",
    "print(\"finshed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655def6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training for real\")\n",
    "train_model(dataset_train,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6287536",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In the vae cell\")\n",
    "vae=keras.models.load_model(\"test_vae_mod_pcen\", custom_objects={\"vae_loss_function\": vae_loss_function})\n",
    "non_normal_scores=error_dataset(vae,dataset_test,False,sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd7a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In here\")\n",
    "bounding_boxes=run_model(non_normal_scores)\n",
    "titles=[\"Begin Time (s)\",\"End Time (s)\",\"Low Freq (Hz)\",\"High Freq (Hz)\",\"Species confidence\"]\n",
    "write_array_to_file(bounding_boxes,titles,predicting_file.split('_')[0] + \"_predictions.txt\")\n",
    "print(\"done modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<_BatchDataset element_spec=TensorSpec(shape=(None, 608, 192, 1), dtype=tf.float64, name=None)>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background_noise = \"avila_filtered.wav\"\n",
    "predicting_file = \"6805.230201070825_processed.wav\"\n",
    "dataset_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T22:35:18.697706Z",
     "start_time": "2024-02-26T22:35:18.672016Z"
    }
   },
   "id": "94ef1934e87fdc17",
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
